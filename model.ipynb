{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSKA: Multi-Stream Keypoint-based Action Recognition for Sign Language (Modified for Hands Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # train_test_split not used directly if using StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "from tqdm import tqdm # Changed from tqdm.notebook\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from einops import rearrange # If used by model components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Path to the directory containing processed .npy keypoint files\n",
    "# Expected structure: data_dir/className/splitName/videoName.npy\n",
    "# where videoName.npy contains holistic keypoints (N_FRAMES, 543, 3)\n",
    "DATA_DIR = \"SignLanguage_Processed_Data/keypoints_for_model\"  # <--- UPDATE THIS PATH\n",
    "NUM_CLASSES = 4  # <--- UPDATE THIS for \"clavier, disque dur, ordinateur, souris\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50 # Adjust as needed, 100 might be long for a start\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "D_MODEL = 128 # Model dimension, can be tuned\n",
    "N_HEAD = 4    # Number of attention heads, can be tuned\n",
    "NUM_LAYERS = 2 # Number of transformer layers, can be tuned\n",
    "\n",
    "# Keypoint indices for hands from holistic (543 keypoints)\n",
    "# Pose: 0-32 (33), Face: 33-500 (468), Left Hand: 501-521 (21), Right Hand: 522-542 (21)\n",
    "LEFT_HAND_INDICES = slice(501, 522) # 21 keypoints\n",
    "RIGHT_HAND_INDICES = slice(522, 543) # 21 keypoints\n",
    "NUM_HAND_KEYPOINTS = 21\n",
    "KEYPOINT_FEATURES = 3 # x, y, z\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Custom PyTorch Dataset for Sign Language Keypoints (Hands Only) ---\n",
    "class SignLanguageKeypointsDataset(Dataset):\n",
    "    def __init__(self, data_samples, labels, max_frames):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_samples (list): List of file paths to .npy files.\n",
    "            labels (list): List of corresponding labels (integers).\n",
    "            max_frames (int): The maximum number of frames to pad/truncate sequences to.\n",
    "        \"\"\"\n",
    "        self.data_samples = data_samples\n",
    "        self.labels = labels\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data_samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            # Load holistic keypoints (N_FRAMES, 543, 3)\n",
    "            holistic_data = np.load(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}. Returning zeros.\")\n",
    "            holistic_data = np.zeros((self.max_frames, 543, KEYPOINT_FEATURES), dtype=np.float32)\n",
    "\n",
    "        # Slice out hand keypoints\n",
    "        # Shape: (N_FRAMES, NUM_HAND_KEYPOINTS, KEYPOINT_FEATURES)\n",
    "        left_hand_kps = holistic_data[:, LEFT_HAND_INDICES, :]\n",
    "        right_hand_kps = holistic_data[:, RIGHT_HAND_INDICES, :]\n",
    "\n",
    "        # Pad or truncate each stream to max_frames\n",
    "        left_hand_kps = self.pad_or_truncate_stream(left_hand_kps, NUM_HAND_KEYPOINTS)\n",
    "        right_hand_kps = self.pad_or_truncate_stream(right_hand_kps, NUM_HAND_KEYPOINTS)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(left_hand_kps, dtype=torch.float32),\n",
    "            torch.tensor(right_hand_kps, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def pad_or_truncate_stream(self, stream, num_keypoints_per_stream):\n",
    "        \"\"\"Pads or truncates a stream (num_frames, num_keypoints, features) to self.max_frames.\"\"\"\n",
    "        current_frames = stream.shape[0]\n",
    "        if current_frames == self.max_frames:\n",
    "            return stream\n",
    "        elif current_frames < self.max_frames:\n",
    "            # Pad with zeros\n",
    "            padding_shape = (self.max_frames - current_frames, num_keypoints_per_stream, KEYPOINT_FEATURES)\n",
    "            padding = np.zeros(padding_shape, dtype=stream.dtype)\n",
    "            return np.concatenate([stream, padding], axis=0)\n",
    "        else:\n",
    "            # Truncate (select first max_frames)\n",
    "            return stream[:self.max_frames, :, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Architecture (Hands Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Components (largely from your original MSKA, adapted) ---\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Attention across keypoints within a single frame for one stream.\"\"\"\n",
    "    def __init__(self, d_model): # d_model is the feature dimension *after* initial projection\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, frames, keypoints, d_model_features]\n",
    "        batch, frames, num_kps, d_model_feat = x.shape\n",
    "        \n",
    "        # Process per-frame: Reshape to treat (batch*frames) as batch for attention\n",
    "        x_reshaped = x.reshape(batch * frames, num_kps, d_model_feat)\n",
    "        \n",
    "        Q = self.query(x_reshaped)\n",
    "        K = self.key(x_reshaped)\n",
    "        V = self.value(x_reshaped)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model_feat**0.5)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        out = torch.matmul(attn_probs, V)\n",
    "        \n",
    "        # Reshape back to original batch and frames dimensions\n",
    "        return out.reshape(batch, frames, num_kps, d_model_feat)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    \"\"\"Processes a temporal sequence of frame-level features using Transformer Encoder.\"\"\"\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward_factor=4):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model * dim_feedforward_factor,\n",
    "            batch_first=True # Important: expects (batch, seq, feature)\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, frames, features (d_model)]\n",
    "        x = self.transformer_encoder(x) # batch_first=True handles this directly\n",
    "        return self.norm(x)\n",
    "\n",
    "class ConvTransformerBlock(nn.Module):\n",
    "    \"\"\"Combines 1D Convolutions with a Temporal Transformer for sequence processing.\"\"\"\n",
    "    def __init__(self, d_model, nhead, num_transformer_layers):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model * 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(d_model * 2, d_model, kernel_size=3, padding=1),\n",
    "        )\n",
    "        self.norm_after_conv = nn.LayerNorm(d_model)\n",
    "        self.transformer = TemporalTransformer(d_model, nhead, num_transformer_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: [batch, frames, features (d_model)]\n",
    "        x_permuted = x.permute(0, 2, 1)  # [batch, features (d_model), frames]\n",
    "        \n",
    "        conv_out = self.conv_block(x_permuted)\n",
    "        conv_out_permuted = conv_out.permute(0, 2, 1) # [batch, frames, features (d_model)]\n",
    "        \n",
    "        # Residual connection + Norm (Pre-norm for Transformer)\n",
    "        x_residual = x + conv_out_permuted \n",
    "        x_normed = self.norm_after_conv(x_residual) \n",
    "        \n",
    "        # Pass to transformer\n",
    "        transformer_out = self.transformer(x_normed)\n",
    "        return transformer_out\n",
    "\n",
    "class HandStreamProcessor(nn.Module):\n",
    "    \"\"\"Processes a single hand keypoint stream (e.g., left or right hand).\"\"\"\n",
    "    def __init__(self, in_keypoint_features, num_keypoints, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        # Projects raw keypoint features (x,y,z) to d_model for each keypoint\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_keypoint_features, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(d_model)\n",
    "        )\n",
    "        self.spatial_attn = SpatialAttention(d_model)\n",
    "        # After spatial attention and mean pooling over keypoints, input to ConvTransformer is d_model\n",
    "        self.temporal_conv_transformer = ConvTransformerBlock(d_model, nhead, num_layers)\n",
    "        # Temporal attention pooling to get a single vector per stream\n",
    "        self.temporal_attn_pool = nn.Linear(d_model, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch, frames, num_keypoints, in_keypoint_features (e.g., 3 for x,y,z)]\n",
    "        x = self.projection(x)  # [batch, frames, num_keypoints, d_model]\n",
    "        x = self.spatial_attn(x) # [batch, frames, num_keypoints, d_model], features are refined\n",
    "        \n",
    "        # Mean pool across keypoints to get a per-frame feature vector\n",
    "        x = x.mean(dim=2)  # [batch, frames, d_model]\n",
    "        \n",
    "        x = self.temporal_conv_transformer(x) # [batch, frames, d_model]\n",
    "        \n",
    "        # Temporal attention pooling\n",
    "        # x is [batch, frames, d_model]\n",
    "        attn_weights = self.temporal_attn_pool(x).squeeze(-1) # [batch, frames]\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1) # [batch, frames]\n",
    "        # Weighted sum: (batch, frames, d_model) * (batch, frames, 1) -> sum over frames\n",
    "        stream_embedding = torch.sum(x * attn_weights.unsqueeze(-1), dim=1) # [batch, d_model]\n",
    "        return stream_embedding\n",
    "\n",
    "class SignLanguageHandsModel(nn.Module):\n",
    "    \"\"\"Model for sign language recognition using only left and right hand keypoints.\"\"\"\n",
    "    def __init__(self, num_classes, in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.left_hand_processor = HandStreamProcessor(\n",
    "            in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers\n",
    "        )\n",
    "        self.right_hand_processor = HandStreamProcessor(\n",
    "            in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers\n",
    "        )\n",
    "        \n",
    "        # Classifier: Takes concatenated features from both hand streams\n",
    "        # Each stream outputs [batch, d_model], so concatenated is [batch, d_model * 2]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model), # Feature fusion layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "       \n",
    "    def forward(self, left_hand_input, right_hand_input):\n",
    "        # left_hand_input: [batch, frames, num_keypoints, features_per_keypoint]\n",
    "        # right_hand_input: [batch, frames, num_keypoints, features_per_keypoint]\n",
    "        \n",
    "        left_out = self.left_hand_processor(left_hand_input)   # [batch, d_model]\n",
    "        right_out = self.right_hand_processor(right_hand_input) # [batch, d_model]\n",
    "        \n",
    "        # Concatenate the features from both hand streams\n",
    "        combined_features = torch.cat([left_out, right_out], dim=-1) # [batch, d_model * 2]\n",
    "        \n",
    "        return self.classifier(combined_features)\n",
    "\n",
    "# --- Stream dimensions (features per keypoint, e.g., x, y, z) ---\n",
    "IN_KEYPOINT_FEATURES = KEYPOINT_FEATURES # Should be 3 (x,y,z)\n",
    "NUM_KP_PER_HAND = NUM_HAND_KEYPOINTS   # Should be 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Loss Function (Balanced Focal Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedFocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Balanced Focal Loss, using effective number of samples for class weighting.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_counts, gamma=2.0, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        if not isinstance(class_counts, torch.Tensor):\n",
    "            class_counts = torch.tensor(class_counts, dtype=torch.float)\n",
    "        \n",
    "        # Effective number of samples calculation\n",
    "        effective_num = 1.0 - torch.pow(beta, class_counts)\n",
    "        # Clamp to avoid division by zero for classes with no samples (should not happen with good data)\n",
    "        weights = (1.0 - beta) / torch.clamp(effective_num, min=1e-8) \n",
    "        weights = weights / weights.sum() * len(class_counts) # Normalize so average weight is 1\n",
    "        self.register_buffer('weights', weights) # Use register_buffer for proper device handling\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # inputs: [batch_size, num_classes] (logits)\n",
    "        # targets: [batch_size] (long tensor of class indices)\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none') # [batch_size]\n",
    "        pt = torch.exp(-ce_loss) # Probabilities of the true class\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        \n",
    "        # Apply class weights based on targets\n",
    "        alpha = self.weights[targets].to(inputs.device) \n",
    "        \n",
    "        balanced_focal_loss = alpha * focal_term * ce_loss\n",
    "        return balanced_focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_classes, device, num_epochs, learning_rate, weight_decay, y_train_labels_for_loss):\n",
    "    # Initialize metrics\n",
    "    train_f1_metric = F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    val_f1_metric = F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    train_accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "    val_accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=5, factor=0.5, verbose=True)\n",
    "\n",
    "    # Calculate class counts for BalancedFocalLoss\n",
    "    if len(y_train_labels_for_loss) == 0:\n",
    "        raise ValueError(\"y_train_labels_for_loss is empty. Ensure data loading provides training labels.\")\n",
    "    class_counts_np = np.bincount(y_train_labels_for_loss, minlength=num_classes)\n",
    "    class_counts_tensor = torch.tensor(class_counts_np, dtype=torch.float).to(device)\n",
    "    criterion = BalancedFocalLoss(class_counts=class_counts_tensor, gamma=2.0)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=False)\n",
    "        \n",
    "        for left_kps, right_kps, labels in train_pbar:\n",
    "            left_kps = left_kps.to(device)\n",
    "            right_kps = right_kps.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(left_kps, right_kps)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            train_f1_metric.update(outputs, labels)\n",
    "            train_accuracy_metric.update(outputs, labels)\n",
    "            train_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"}) \n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "        epoch_train_f1 = train_f1_metric.compute().item()\n",
    "        epoch_train_acc = train_accuracy_metric.compute().item()\n",
    "        train_f1_metric.reset()\n",
    "        train_accuracy_metric.reset()\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for left_kps, right_kps, labels in val_pbar:\n",
    "                left_kps = left_kps.to(device)\n",
    "                right_kps = right_kps.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(left_kps, right_kps)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                epoch_val_loss += loss.item()\n",
    "                val_f1_metric.update(outputs, labels)\n",
    "                val_accuracy_metric.update(outputs, labels)\n",
    "                val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader)\n",
    "        epoch_val_f1 = val_f1_metric.compute().item()\n",
    "        epoch_val_acc = val_accuracy_metric.compute().item()\n",
    "        val_f1_metric.reset()\n",
    "        val_accuracy_metric.reset()\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_f1'].append(epoch_train_f1)\n",
    "        history['val_f1'].append(epoch_val_f1)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train F1: {epoch_train_f1:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f}, Val F1:   {epoch_val_f1:.4f}, Val Acc:   {epoch_val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(epoch_val_f1) \n",
    "\n",
    "        if epoch_val_f1 > best_val_f1:\n",
    "            best_val_f1 = epoch_val_f1\n",
    "            torch.save(model.state_dict(), 'best_sign_model_hands_only.pth')\n",
    "            print(f\"  üî• New best model saved with Val F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history['train_f1'], label='Train F1')\n",
    "    plt.plot(history['val_f1'], label='Val F1')\n",
    "    plt.title('F1 Score Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Val Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sign_language_training_hands_only.png')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_sign_model_hands_only.pth')) \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Evaluation on Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, test_loader, device, label_mapping_dict, num_classes_eval):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    test_pbar = tqdm(test_loader, desc='Evaluating on Test Set', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for left_kps, right_kps, labels in test_pbar:\n",
    "            left_kps = left_kps.to(device)\n",
    "            right_kps = right_kps.to(device)\n",
    "            \n",
    "            outputs = model(left_kps, right_kps)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    unique_labels_in_test = sorted(list(set(all_labels)))\n",
    "    target_names_report = [name for name, idx in sorted(label_mapping_dict.items(), key=lambda item: item[1]) if idx in unique_labels_in_test]\n",
    "    \n",
    "    cm_target_names = [name for name, idx in sorted(label_mapping_dict.items(), key=lambda item: item[1])]\n",
    "    cm_labels = list(range(num_classes_eval))\n",
    "\n",
    "    print(\"\\n--- Test Set Evaluation ---\")\n",
    "    print(\"\\nüìä Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, labels=unique_labels_in_test, target_names=target_names_report, digits=4, zero_division=0))\n",
    "    \n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    test_f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\nüîç Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"üîç Test F1-Score (Macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"üîç Test F1-Score (Weighted): {test_f1_weighted:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=cm_labels)\n",
    "    plt.figure(figsize=(max(10, num_classes_eval // 1.5), max(8, num_classes_eval // 2))) \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=cm_target_names,\n",
    "                yticklabels=cm_target_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix_hands_only.png')\n",
    "    \n",
    "    return {'accuracy': test_accuracy, 'f1_macro': test_f1_macro, 'f1_weighted': test_f1_weighted}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Model Instantiation and Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # --- Load data paths and labels ---\n",
    "    all_file_paths = []\n",
    "    all_labels = []\n",
    "    label_mapping = {} # Will be populated here\n",
    "    current_label_idx = 0\n",
    "    frame_counts = []\n",
    "\n",
    "    print(f\"Loading data from: {DATA_DIR}\")\n",
    "    if not os.path.exists(DATA_DIR):\n",
    "        raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}. Please run preprocessing first.\")\n",
    "        \n",
    "    for class_name in sorted(os.listdir(DATA_DIR)):\n",
    "        class_path = os.path.join(DATA_DIR, class_name)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        if class_name not in label_mapping:\n",
    "            label_mapping[class_name] = current_label_idx\n",
    "            current_label_idx += 1\n",
    "        class_label = label_mapping[class_name]\n",
    "\n",
    "        for split_name in sorted(os.listdir(class_path)): # e.g., 'train', 'val', 'test'\n",
    "            split_path = os.path.join(class_path, split_name)\n",
    "            if not os.path.isdir(split_path):\n",
    "                continue\n",
    "\n",
    "            for file_name in os.listdir(split_path):\n",
    "                if file_name.endswith(\".npy\"):\n",
    "                    file_path = os.path.join(split_path, file_name)\n",
    "                    all_file_paths.append(file_path)\n",
    "                    all_labels.append(class_label)\n",
    "                    try:\n",
    "                        data = np.load(file_path)\n",
    "                        frame_counts.append(data.shape[0])\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not load {file_path} to get frame count: {e}\")\n",
    "                        frame_counts.append(0) \n",
    "\n",
    "    if not all_file_paths:\n",
    "        raise ValueError(f\"No .npy files found in {DATA_DIR}. Please check the path and data structure.\")\n",
    "\n",
    "    if frame_counts:\n",
    "        MAX_FRAMES = int(np.percentile(frame_counts, 95)) \n",
    "        print(f\"Using MAX_FRAMES = {MAX_FRAMES} (95th percentile of frame counts)\")\n",
    "    else:\n",
    "        MAX_FRAMES = 50 \n",
    "        print(f\"Warning: No frame counts. Using default MAX_FRAMES = {MAX_FRAMES}\")\n",
    "\n",
    "    X = np.array(all_file_paths)\n",
    "    y = np.array(all_labels)\n",
    "\n",
    "    # --- Stratified Train-Validation-Test Split ---\n",
    "    sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_val_indices, test_indices = next(sss_test.split(X, y))\n",
    "    X_train_val, X_test_paths = X[train_val_indices], X[test_indices]\n",
    "    y_train_val, y_test_labels = y[train_val_indices], y[test_indices]\n",
    "\n",
    "    sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42) \n",
    "    train_indices, val_indices = next(sss_val.split(X_train_val, y_train_val))\n",
    "    X_train_paths, X_val_paths = X_train_val[train_indices], X_train_val[val_indices]\n",
    "    y_train_labels, y_val_labels = y_train_val[train_indices], y_train_val[val_indices] # y_train_labels is now defined\n",
    "\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    print(f\"Train samples: {len(X_train_paths)}\")\n",
    "    print(f\"Validation samples: {len(X_val_paths)}\")\n",
    "    print(f\"Test samples: {len(X_test_paths)}\")\n",
    "\n",
    "    train_dataset = SignLanguageKeypointsDataset(X_train_paths, y_train_labels, MAX_FRAMES)\n",
    "    val_dataset = SignLanguageKeypointsDataset(X_val_paths, y_val_labels, MAX_FRAMES)\n",
    "    test_dataset = SignLanguageKeypointsDataset(X_test_paths, y_test_labels, MAX_FRAMES)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "    \n",
    "    print(\"\\nClass distribution in splits:\")\n",
    "    for split_name, labels_in_split_arg in [('Train', y_train_labels), ('Validation', y_val_labels), ('Test', y_test_labels)]:\n",
    "        counts = Counter(labels_in_split_arg)\n",
    "        print(f\"{split_name}:\")\n",
    "        for label_idx, count in sorted(counts.items()):\n",
    "            class_name_found = [name for name, idx in label_mapping.items() if idx == label_idx]\n",
    "            if class_name_found:\n",
    "                class_name = class_name_found[0]\n",
    "                print(f\"  {class_name} (ID {label_idx}): {count}\")\n",
    "            else:\n",
    "                print(f\"  Unknown Label ID {label_idx}: {count}\")\n",
    "\n",
    "    # Initialize the model\n",
    "    model_instance = SignLanguageHandsModel(\n",
    "        num_classes=NUM_CLASSES, \n",
    "        in_keypoint_features=IN_KEYPOINT_FEATURES, \n",
    "        num_hand_keypoints=NUM_KP_PER_HAND, \n",
    "        d_model=D_MODEL, \n",
    "        nhead=N_HEAD, \n",
    "        num_layers=NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    print(f\"Model instantiated with {sum(p.numel() for p in model_instance.parameters() if p.requires_grad)} trainable parameters.\")\n",
    "\n",
    "    trained_model_instance = None\n",
    "    if train_loader and val_loader:\n",
    "        trained_model_instance, history_data = train_model(\n",
    "            model=model_instance,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            device=DEVICE,\n",
    "            num_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            y_train_labels_for_loss=y_train_labels # Pass y_train_labels here\n",
    "        )\n",
    "    else:\n",
    "        print(\"Dataloaders not initialized. Skipping training.\")\n",
    "\n",
    "    # Evaluation\n",
    "    if trained_model_instance and test_loader:\n",
    "        print(\"\\nEvaluating the model trained in this session.\")\n",
    "        evaluation_results = evaluate_model(trained_model_instance, test_loader, DEVICE, label_mapping, NUM_CLASSES)\n",
    "    elif os.path.exists('best_sign_model_hands_only.pth') and test_loader:\n",
    "        print(\"\\nLoading best model from 'best_sign_model_hands_only.pth' for evaluation.\")\n",
    "        model_to_evaluate_loaded = SignLanguageHandsModel(\n",
    "            num_classes=NUM_CLASSES, \n",
    "            in_keypoint_features=IN_KEYPOINT_FEATURES, \n",
    "            num_hand_keypoints=NUM_KP_PER_HAND, \n",
    "            d_model=D_MODEL, \n",
    "            nhead=N_HEAD, \n",
    "            num_layers=NUM_LAYERS\n",
    "        ).to(DEVICE)\n",
    "        try:\n",
    "            model_to_evaluate_loaded.load_state_dict(torch.load('best_sign_model_hands_only.pth', map_location=DEVICE))\n",
    "            evaluation_results = evaluate_model(model_to_evaluate_loaded, test_loader, DEVICE, label_mapping, NUM_CLASSES)\n",
    "        except FileNotFoundError:\n",
    "            print(\"Error: 'best_sign_model_hands_only.pth' not found during evaluation phase.\")\n",
    "    else:\n",
    "        print(\"No trained model or test_loader available. Skipping evaluation.\")\n",
    "\n",
    "    if 'plt' in globals():\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
