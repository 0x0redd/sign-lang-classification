\documentclass[11pt, a4paper]{article}

% --- PREAMBLE (Packages for formatting, math, images, code, etc.) ---
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\usepackage[margin=1in]{geometry} % Adjust margins as needed
\usepackage{hyperref}
\usepackage{listings} % For code blocks
\usepackage{booktabs} % For professional tables
\usepackage{caption}  % For better caption control
\usepackage{float}    % For [H] figure placement
\usepackage{xcolor}   % For colored text/boxes if needed
\usepackage{enumitem} % For customized lists

% --- LISTINGS CONFIGURATION (for code blocks) ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{lightgray!10},
    commentstyle=\color{green!50!black},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{gray},
    stringstyle=\color{purple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}
\lstset{style=mystyle}

% --- HYPERREF SETUP ---
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Sign Language Classification using Keypoints and Sequence Models},
    pdfauthor={Othmane Ferrah, Hicham Benlmahi, Soufian El Karchal},
    pdfsubject={Artificial Intelligence, Computer Vision, Machine Learning},
    pdfkeywords={Sign Language Recognition, Action Recognition, YOLO, MediaPipe, LSTM, PyTorch Lightning},
}

% --- DOCUMENT START ---
\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
\centering

{\large Department of Computer Science and Engineering}\\[0.5cm]
{\large Licence Program: Mathematical Sciences and Computer Science (SMI)}\\[2cm]

{\Huge\bfseries End-of-studies Project}\\[1cm]

{\Large On the topic:}\\[0.5cm]
{\huge\bfseries Development of a Real-Time Sign Language\\
Recognition System Using Deep Learning\\
and Computer Vision}\\[2cm]

{\large Presented by:}\\[0.5cm]
{\Large\bfseries Othmane FERRAH}\\
{\Large\bfseries Hicham BENLMAHI}\\
{\Large\bfseries Soufian EL KARCHAL}\\[1.5cm]

{\large Supervised by:}\\[0.5cm]
{\Large Pr. [Supervisor Name]}\\[1.5cm]

{\large Defended on [Date], before the jury:}\\[0.5cm]
Pr. [Name] : Professor at [Institution]\\
Pr. [Name] : Professor at [Institution]\\
Pr. [Name] : Professor at [Institution]\\[2cm]

{\large Academic Year 2024/2025}

\end{titlepage}

% --- ACKNOWLEDGEMENTS ---
\section*{Acknowledgements}
We would like to express our deepest gratitude to everyone who contributed to the successful completion of this project.

First and foremost, we extend our sincere thanks to our supervisor, Pr. [Supervisor Name], for their unwavering support, guidance, and insightful feedback throughout the duration of this project. Their expertise, patience, and encouragement have been instrumental in shaping both the direction and quality of this research, and for that, we are truly grateful.

We would also like to extend our sincere appreciation to the distinguished members of the jury, who will dedicate their time and effort to evaluating this report. Their critical insights and feedback will undoubtedly enhance the quality of this work.

Our heartfelt thanks go to [University Name], the Faculty of Sciences, and all the professors of the Licence Program in SMI: Mathematical Sciences and Computer Science. Their collective efforts in providing knowledge, resources, and academic guidance have been crucial in helping us achieve our academic goals throughout our undergraduate studies.

Finally, our deepest gratitude is reserved for our family and friends, whose unwavering support and encouragement have been our greatest source of strength. Their belief in us has been invaluable in overcoming the challenges faced during our undergraduate studies. We truly could not have reached this point without them.

\newpage

% --- ABSTRACT ---
\begin{abstract}
In the field of human-computer interaction and accessibility technology, the accurate recognition of sign language gestures is a critical task that significantly impacts communication accessibility for the deaf and hard-of-hearing communities. This report presents a comprehensive exploration of deep learning techniques, specifically focusing on the application of YOLOv8 for hand detection combined with MediaPipe for keypoint extraction, followed by LSTM-based sequence classification for multi-class sign language recognition using video data. By leveraging the real-time processing capabilities of the YOLO architecture, advanced keypoint extraction methodologies, and sequence modeling with recurrent neural networks, this work achieves high levels of accuracy in classifying four distinct sign language gestures: clavier (keyboard), disque\_dur (hard drive), ordinateur (computer), and souris (mouse).

The dataset was significantly expanded through a comprehensive preprocessing pipeline including hand detection, keypoint normalization, sequence padding/truncation, and temporal feature extraction. The model was assessed using standard evaluation metrics including accuracy, precision, recall, and F1-score, achieving an overall test accuracy of 80\% with a macro F1-score of 0.79.

Implemented as both a trained model pipeline and a standalone web application using Flask, the system provides users with an intuitive interface for real-time video analysis without requiring specialized hardware or cloud dependencies. This offline capability ensures data privacy compliance while enabling efficient gesture recognition in diverse environments. The web interface supports both video upload and live recording functionality, displaying prediction results with confidence scores for practical deployment.

This research contributes to the growing field of AI-assisted accessibility technology while demonstrating the potential of combining computer vision and sequence modeling techniques to create scalable, real-time sign language recognition solutions. Future directions include dataset expansion, model architecture enhancement, and integration of additional sign language vocabularies.
\end{abstract}

% --- RÉSUMÉ ---
\begin{abstract}
\selectlanguage{french}
Dans le domaine de l'interaction homme-machine et des technologies d'accessibilité, la reconnaissance précise des gestes de la langue des signes est une tâche critique qui impacte significativement l'accessibilité de la communication pour les communautés sourdes et malentendantes. Ce rapport présente une exploration approfondie des techniques d'apprentissage profond, en se concentrant spécifiquement sur l'application de YOLOv8 pour la détection des mains combinée avec MediaPipe pour l'extraction de points clés, suivie d'une classification de séquences basée sur LSTM pour la reconnaissance multi-classes de la langue des signes utilisant des données vidéo.

Le jeu de données a été significativement élargi grâce à un pipeline de prétraitement complet incluant la détection des mains, la normalisation des points clés, le remplissage/troncature des séquences, et l'extraction de caractéristiques temporelles. Le modèle a été évalué en utilisant des métriques d'évaluation standards incluant l'exactitude, la précision, le rappel et le F1-score, atteignant une précision de test globale de 80\% avec un F1-score macro de 0,79.

Implémenté à la fois comme un pipeline de modèle entraîné et une application web autonome utilisant Flask, le système fournit aux utilisateurs une interface intuitive pour l'analyse vidéo en temps réel sans nécessiter de matériel spécialisé ou de dépendances cloud. Cette recherche contribue au domaine croissant des technologies d'accessibilité assistées par l'IA tout en démontrant le potentiel de combiner la vision par ordinateur et les techniques de modélisation de séquences pour créer des solutions de reconnaissance de langue des signes évolutives et en temps réel.
\end{abstract}
\selectlanguage{english}

% --- TABLE OF CONTENTS ---
\tableofcontents
\newpage

% --- LIST OF FIGURES ---
\listoffigures
\newpage

% --- LIST OF TABLES ---
\listoftables
\newpage

% --- LIST OF LISTINGS ---
\renewcommand\lstlistlistingname{List of Listings}
\lstlistoflistings
\newpage

% --- LIST OF ABBREVIATIONS ---
\section*{List of Abbreviations}
\begin{tabular}{ll}
AI & Artificial Intelligence \\
API & Application Programming Interface \\
ASL & American Sign Language \\
CNN & Convolutional Neural Network \\
CV & Computer Vision \\
DL & Deep Learning \\
FNR & False Negative Rate \\
FPR & False Positive Rate \\
GPU & Graphics Processing Unit \\
GUI & Graphical User Interface \\
HDF5 & Hierarchical Data Format 5 \\
HTML & HyperText Markup Language \\
HTTP & HyperText Transfer Protocol \\
LSTM & Long Short-Term Memory \\
ML & Machine Learning \\
MP4 & Moving Picture Experts Group Layer-4 \\
PyTorch & Python Torch \\
RGB & Red Green Blue \\
RNN & Recurrent Neural Network \\
ROI & Region of Interest \\
SLR & Sign Language Recognition \\
SMI & Sciences Mathématiques et Informatique \\
UI & User Interface \\
YOLO & You Only Look Once \\
\end{tabular}
\newpage

% --- SECTIONS ---

\section{General Introduction}

\subsection{Context}

Sign language is a visual language that serves as the primary means of communication for millions of deaf and hard-of-hearing individuals worldwide. It utilizes hand gestures, facial expressions, and body movements to convey meaning, forming complex linguistic systems that are as rich and expressive as spoken languages. The recognition and interpretation of sign language through artificial intelligence represents a critical advancement in accessibility technology, with the potential to bridge communication gaps and create more inclusive digital environments.

The emergence of artificial intelligence (AI) and computer vision has opened new possibilities for automated sign language recognition systems. Traditional approaches to sign language interpretation have relied heavily on human interpreters, which, while effective, are not always available and can be costly for widespread deployment. AI-powered systems offer the promise of real-time, accessible, and scalable solutions that can operate independently of human intervention~\cite{Adaloglou21}.

Recent advances in deep learning, particularly in computer vision and sequence modeling, have demonstrated remarkable capabilities in understanding temporal visual patterns. The combination of object detection algorithms like YOLO (You Only Look Once) with pose estimation frameworks such as MediaPipe, followed by sequence classification using recurrent neural networks, has shown significant promise in gesture and action recognition tasks~\cite{Camgoz20}.

\subsection{Problem Statement}

Despite significant progress in computer vision and machine learning, developing a reliable and accurate sign language recognition system remains a challenging endeavor. The complexity of sign language recognition stems from several key factors:

\begin{enumerate}
    \item \textbf{Temporal Complexity:} Sign language gestures are inherently temporal, requiring the analysis of movement patterns over time rather than static image classification.
    
    \item \textbf{Spatial Variability:} Hand positions, orientations, and scales can vary significantly between individuals and recording conditions, making robust feature extraction challenging.
    
    \item \textbf{Limited Datasets:} Unlike other computer vision tasks, large-scale, well-annotated sign language datasets are relatively scarce, limiting the training potential of deep learning models.
    
    \item \textbf{Real-time Processing Requirements:} Practical applications require real-time inference capabilities while maintaining high accuracy.
    
    \item \textbf{Accessibility and Deployment:} Systems must be deployable in diverse environments with varying computational resources and internet connectivity.
\end{enumerate}

These challenges underscore the need for robust AI systems capable of handling the nuances of sign language recognition while remaining practical for real-world deployment~\cite{Koller20}.

\subsection{Objectives}

The primary objectives of this project are:

\begin{enumerate}
    \item \textbf{Develop a Comprehensive Sign Language Recognition Pipeline:} To design and implement a complete system that processes raw video input through hand detection, keypoint extraction, sequence modeling, and classification to recognize sign language gestures.
    
    \item \textbf{Implement State-of-the-Art Computer Vision Techniques:} To leverage YOLOv8 for robust hand detection and MediaPipe for accurate keypoint extraction, creating a reliable preprocessing pipeline for temporal data.
    
    \item \textbf{Design an Effective Sequence Classification Model:} To develop and train an LSTM-based neural network capable of learning temporal patterns in keypoint sequences for accurate gesture classification.
    
    \item \textbf{Create a Practical Deployment Solution:} To implement both a complete training pipeline and a user-friendly web application that enables real-time sign language recognition for end users.
    
    \item \textbf{Evaluate and Validate System Performance:} To conduct comprehensive evaluation using standard machine learning metrics and demonstrate the system's effectiveness through real-world testing scenarios.
    
    \item \textbf{Ensure Accessibility and Privacy:} To develop an offline-capable system that respects user privacy while maintaining high performance and usability standards.
\end{enumerate}

\subsection{Structure of the Report}

This report is organized as follows:

\begin{itemize}
    \item \textbf{Chapter 1: General Introduction} - Presents the context, problem statement, objectives, and report structure.
    
    \item \textbf{Chapter 2: Theoretical Background and State of the Art} - Reviews the fundamental concepts in computer vision, deep learning, and sequence modeling, along with current approaches to sign language recognition.
    
    \item \textbf{Chapter 3: Data Acquisition and Preprocessing} - Details the dataset structure, video preprocessing pipeline, keypoint extraction methodology, and sequence preparation techniques.
    
    \item \textbf{Chapter 4: Model Architecture and Training} - Describes the LSTM-based sequence classification model, training procedures using PyTorch Lightning, and optimization strategies.
    
    \item \textbf{Chapter 5: Implementation and Results} - Presents the complete system implementation, evaluation metrics, experimental results, and performance analysis.
    
    \item \textbf{Chapter 6: Web Application Development} - Details the development of the user-facing web application, including frontend design and backend integration.
    
    \item \textbf{Chapter 7: Discussion} - Analyzes the results, discusses limitations, and provides insights for future improvements.
    
    \item \textbf{Chapter 8: Conclusion} - Summarizes the achievements, contributions, and future perspectives.
    
    \item \textbf{Bibliography} - Lists all references cited throughout the report.
\end{itemize}

\section{Theoretical Background and State of the Art}

\subsection{Deep Learning Fundamentals}

\subsubsection{Definition and Core Concepts}

Deep learning (DL) is a subfield of machine learning that employs artificial neural networks with multiple layers (deep architectures) to model complex patterns in data. Unlike traditional machine learning approaches, which rely heavily on manual feature engineering, deep learning automates feature extraction through hierarchical learning, enabling it to excel in tasks such as image recognition, natural language processing, and temporal sequence analysis~\cite{Goodfellow16}.

The fundamental principle of deep learning lies in its ability to learn hierarchical representations of data through multiple layers of non-linear transformations. Each layer in a deep network learns increasingly abstract features, from low-level patterns (edges, textures) in early layers to high-level semantic concepts in deeper layers~\cite{LeCun15}.

\subsubsection{Computer Vision in Deep Learning}

Computer vision (CV) represents one of the most successful applications of deep learning, enabling computers to interpret and understand visual information from images and videos. The cornerstone of deep learning in computer vision is the Convolutional Neural Network (CNN), which utilizes layers of convolutional filters to learn spatial hierarchies of features directly from pixel data~\cite{Krizhevsky12}.

Key applications relevant to this project include:

\begin{itemize}
    \item \textbf{Object Detection:} Identifying and localizing objects within images, such as detecting hands for sign language recognition.
    \item \textbf{Pose Estimation:} Determining the spatial configuration of body parts, crucial for extracting hand keypoints.
    \item \textbf{Action Recognition:} Understanding temporal patterns in video sequences, essential for recognizing sign language gestures.
\end{itemize}

\subsubsection{Sequence Modeling and Recurrent Neural Networks}

Sequence modeling addresses the challenge of learning patterns in temporal data, where the order and timing of information are crucial. Recurrent Neural Networks (RNNs) and their advanced variants, Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), are specifically designed to handle sequential data by maintaining internal memory states~\cite{Hochreiter97}.

The LSTM architecture addresses the vanishing gradient problem inherent in traditional RNNs through a system of gates that regulate information flow:

\begin{equation}
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
\end{equation}

\begin{equation}
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\end{equation}

\begin{equation}
\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)
\end{equation}

\begin{equation}
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
\end{equation}

\begin{equation}
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
\end{equation}

\begin{equation}
h_t = o_t * \tanh(C_t)
\end{equation}

Where $f_t$, $i_t$, and $o_t$ represent the forget, input, and output gates respectively, $C_t$ is the cell state, and $h_t$ is the hidden state at time step $t$.

\subsection{Sign Language Recognition: State of the Art}

\subsubsection{Evolution of Approaches}

Sign language recognition has evolved through several distinct phases, each characterized by different technological approaches and capabilities:

\textbf{Traditional Computer Vision Approaches (Pre-2015):}
Early systems relied on handcrafted features and classical machine learning algorithms. These approaches typically involved:
\begin{itemize}
    \item Manual feature extraction (HOG, SIFT, optical flow)
    \item Support Vector Machines (SVM) or Hidden Markov Models (HMM) for classification
    \item Limited vocabulary and controlled environments
    \item Accuracy rates typically ranging from 60-80\%~\cite{Ong05}
\end{itemize}

\textbf{Deep Learning Revolution (2015-Present):}
The introduction of deep learning transformed sign language recognition capabilities:
\begin{itemize}
    \item CNN-based feature extraction replacing handcrafted features
    \item RNN/LSTM networks for temporal modeling
    \item Increased vocabulary sizes and improved robustness
    \item Accuracy improvements to 85-95\% in controlled settings~\cite{Huang18}
\end{itemize}

\subsubsection{Current Approaches and Architectures}

Modern sign language recognition systems typically employ one of several architectural paradigms:

\textbf{3D Convolutional Networks:}
3D CNNs process spatio-temporal information directly from video data, learning both spatial and temporal features simultaneously. While effective, these approaches are computationally intensive and require large amounts of training data~\cite{Tran15}.

\textbf{Two-Stream Networks:}
These architectures process spatial (RGB frames) and temporal (optical flow) information separately before fusion. This approach balances computational efficiency with temporal modeling capabilities~\cite{Simonyan14}.

\textbf{Keypoint-Based Approaches:}
Recent work has focused on extracting skeletal keypoints (especially hand landmarks) and processing these lower-dimensional representations with sequence models. This approach offers several advantages:
\begin{itemize}
    \item Reduced computational requirements
    \item Improved generalization across different backgrounds and lighting conditions
    \item More interpretable feature representations
    \item Faster inference times suitable for real-time applications~\cite{Adaloglou21}
\end{itemize}

\subsection{Key Technical Challenges}

\subsubsection{Data Scarcity and Quality}

Unlike other computer vision domains, sign language recognition suffers from limited high-quality, annotated datasets. Most available datasets are relatively small (hundreds to thousands of samples) compared to the millions of samples typical in other vision tasks. This scarcity poses significant challenges for training robust deep learning models~\cite{Bragg19}.

\subsubsection{Temporal Modeling Complexity}

Sign language gestures exhibit complex temporal patterns, including:
\begin{itemize}
    \item Variable gesture duration
    \item Co-articulation effects between consecutive signs
    \item Individual signing style variations
    \item Speed and rhythm differences~\cite{Camgoz20}
\end{itemize}

\subsubsection{Real-time Processing Requirements}

Practical applications demand real-time processing capabilities, typically requiring inference times under 100ms per gesture. This constraint limits the complexity of models that can be deployed in production environments~\cite{Koller20}.

\subsection{MediaPipe and Modern Pose Estimation}

MediaPipe, developed by Google, represents a significant advancement in real-time pose estimation and hand tracking. The framework provides robust hand landmark detection with 21 keypoints per hand, offering sub-pixel accuracy and real-time performance on standard hardware~\cite{Zhang20}.

The MediaPipe hand model processes images through several stages:
\begin{enumerate}
    \item Palm detection using a lightweight CNN
    \item Hand landmark regression within detected palm regions
    \item 3D hand pose estimation and tracking across frames
\end{enumerate}

This approach enables reliable keypoint extraction even in challenging conditions such as varying lighting, backgrounds, and hand orientations.

\subsection{YOLO Architecture for Object Detection}

The YOLO (You Only Look Once) family of object detection algorithms has revolutionized real-time object detection through their unified approach to bounding box prediction and classification. YOLOv8, the latest iteration, offers improved accuracy and speed compared to previous versions while maintaining the single-shot detection paradigm~\cite{Ultralytics23}.

For sign language recognition, YOLO serves a crucial role in detecting regions of interest (hands, persons) before applying more computationally intensive pose estimation algorithms. This two-stage approach significantly improves both accuracy and efficiency compared to processing entire image frames.

\subsection{Summary}

The current state of sign language recognition demonstrates significant progress through the integration of modern computer vision and deep learning techniques. The combination of robust object detection, accurate pose estimation, and effective sequence modeling provides a strong foundation for developing practical sign language recognition systems. However, challenges remain in areas of data availability, real-time processing, and generalization across diverse conditions and users.

\section{Data Acquisition and Preprocessing}

\subsection{Dataset Description and Structure}

\subsubsection{Dataset Composition}

The dataset employed in this project consists of video recordings of four distinct sign language gestures representing computer-related terms in French Sign Language (LSF). The dataset was structured to support multi-class classification with the following categories:

\begin{itemize}
    \item \textbf{clavier} (keyboard): 50 video samples
    \item \textbf{disque\_dur} (hard drive): Various video samples
    \item \textbf{ordinateur} (computer): Various video samples  
    \item \textbf{souris} (mouse): Various video samples
\end{itemize}

The total dataset comprises 199 video files organized in a hierarchical directory structure:

\begin{verbatim}
SignLanguageDataset/
├── clavier/
│   ├── clavier1.mp4
│   ├── clavier2.mp4
│   └── ...
├── disque_dur/
│   ├── disque_dur1.mp4
│   ├── disque_dur2.mp4
│   └── ...
├── ordinateur/
│   └── ...
└── souris/
    └── ...
\end{verbatim}

\subsubsection{Video Characteristics}

Each video sample exhibits the following characteristics:
\begin{itemize}
    \item Duration: Variable length (typically 2-5 seconds)
    \item Resolution: Variable (standardized to 640×640 during preprocessing)
    \item Frame rate: Variable (processed at fixed intervals)
    \item Format: MP4 with H.264 encoding
    \item Content: Single individual performing isolated sign gestures
\end{itemize}

\subsection{Preprocessing Pipeline Architecture}

\subsubsection{Overview of the Preprocessing Workflow}

The preprocessing pipeline transforms raw video data into structured numerical sequences suitable for machine learning model training. The workflow consists of several sequential stages:

\begin{enumerate}
    \item \textbf{Video Frame Extraction:} Decomposition of video files into individual frames
    \item \textbf{Hand Detection:} Application of YOLOv8 for region of interest identification
    \item \textbf{Keypoint Extraction:} MediaPipe-based hand landmark detection
    \item \textbf{Feature Normalization:} Coordinate normalization and standardization
    \item \textbf{Sequence Formation:} Temporal aggregation and length standardization
    \item \textbf{Data Storage:} Efficient storage in HDF5 format for training
\end{enumerate}

\subsubsection{Hand Detection using YOLOv8}

The YOLOv8 object detection framework serves as the initial stage of the preprocessing pipeline, responsible for identifying regions of interest containing hands or persons within each video frame. This approach offers several advantages:

\begin{itemize}
    \item \textbf{Computational Efficiency:} By focusing subsequent processing on relevant image regions
    \item \textbf{Noise Reduction:} Elimination of background artifacts and irrelevant visual information
    \item \textbf{Improved Accuracy:} Enhanced precision of downstream keypoint detection
\end{itemize}

The YOLOv8 detection process can be formulated as:

\begin{equation}
\mathbf{B}, \mathbf{C}, \mathbf{S} = f_{YOLO}(\mathbf{I})
\end{equation}

Where:
\begin{itemize}
    \item $\mathbf{I} \in \mathbb{R}^{H \times W \times 3}$ represents the input image
    \item $\mathbf{B} \in \mathbb{R}^{N \times 4}$ contains bounding box coordinates for $N$ detections
    \item $\mathbf{C} \in \mathbb{R}^{N \times K}$ represents class probabilities for $K$ classes
    \item $\mathbf{S} \in \mathbb{R}^{N}$ contains confidence scores for each detection
\end{itemize}

Detection filtering is applied using a confidence threshold $\tau_{conf} = 0.4$:

\begin{equation}
\mathbf{B}_{filtered} = \{\mathbf{b}_i | s_i > \tau_{conf}\}
\end{equation}

\subsubsection{Keypoint Extraction using MediaPipe}

Following hand detection, the MediaPipe Hands solution extracts precise hand landmarks within the identified regions of interest. MediaPipe provides 21 anatomically meaningful keypoints per hand, representing major joints and fingertips.

The keypoint extraction process operates on detected regions $\mathbf{R}_i$ and produces normalized landmark coordinates:

\begin{equation}
\mathbf{L}_i = g_{MediaPipe}(\mathbf{R}_i)
\end{equation}

Where $\mathbf{L}_i \in \mathbb{R}^{21 \times 3}$ contains $(x, y, z)$ coordinates for 21 landmarks, with coordinates normalized to the range $[0, 1]$ relative to the input image dimensions.

\subsubsection{Feature Vector Construction}

For each frame $t$ in a video sequence, the feature vector $\mathbf{x}_t$ is constructed by concatenating normalized keypoint coordinates from detected hands:

\begin{equation}
\mathbf{x}_t = [\mathbf{L}_{left}^{(x)}, \mathbf{L}_{left}^{(y)}, \mathbf{L}_{right}^{(x)}, \mathbf{L}_{right}^{(y)}] \in \mathbb{R}^{84}
\end{equation}

Where:
\begin{itemize}
    \item $\mathbf{L}_{left}^{(x)}, \mathbf{L}_{left}^{(y)} \in \mathbb{R}^{21}$ represent left hand x,y coordinates
    \item $\mathbf{L}_{right}^{(x)}, \mathbf{L}_{right}^{(y)} \in \mathbb{R}^{21}$ represent right hand x,y coordinates
    \item Missing hands are represented by zero vectors
\end{itemize}

The resulting feature dimensionality is:
\begin{equation}
d_{feature} = N_{hands} \times N_{keypoints} \times N_{coordinates} = 2 \times 21 \times 2 = 84
\end{equation}

\subsubsection{Sequence Length Standardization}

To ensure consistent input dimensions for the sequence classification model, all video sequences are standardized to a fixed length $T = 30$ frames through padding or truncation:

\textbf{Truncation (for sequences longer than $T$):}
\begin{equation}
\mathbf{X}_{truncated} = \mathbf{X}_{start:start+T}
\end{equation}

Where $start = \frac{|\mathbf{X}| - T}{2}$ for center cropping.

\textbf{Padding (for sequences shorter than $T$):}
\begin{equation}
\mathbf{X}_{padded} = [\mathbf{X}, \mathbf{0}_{(T-|\mathbf{X}|) \times d_{feature}}]
\end{equation}

The final sequence representation is:
\begin{equation}
\mathbf{X} \in \mathbb{R}^{T \times d_{feature}} = \mathbb{R}^{30 \times 84}
\end{equation}

\subsection{Data Storage and Organization}

\subsubsection{HDF5 Storage Format}

Processed sequences are stored using the Hierarchical Data Format 5 (HDF5), which provides several advantages for machine learning applications:

\begin{itemize}
    \item \textbf{Efficient Storage:} Compressed binary format reducing storage requirements
    \item \textbf{Fast Access:} Random access capabilities for efficient batch loading
    \item \textbf{Metadata Support:} Storage of labels and preprocessing parameters
    \item \textbf{Cross-platform Compatibility:} Standard format supported across platforms
\end{itemize}

\subsubsection{Dataset Splitting Strategy}

The preprocessed dataset is partitioned into training, validation, and test sets using stratified sampling to maintain class distribution:

\begin{equation}
\mathcal{D} = \mathcal{D}_{train} \cup \mathcal{D}_{val} \cup \mathcal{D}_{test}
\end{equation}

With proportions:
\begin{itemize}
    \item Training set: 70\% (139 samples)
    \item Validation set: 20\% (40 samples)
    \item Test set: 10\% (20 samples)
\end{itemize}

Stratification ensures that each subset maintains the original class distribution $p(y = c)$ for all classes $c \in \{clavier, disque\_dur, ordinateur, souris\}$.

\subsection{Quality Assurance and Validation}

\subsubsection{Preprocessing Validation}

Several validation steps ensure the quality and consistency of the preprocessing pipeline:

\begin{enumerate}
    \item \textbf{Keypoint Detection Rate:} Verification that MediaPipe successfully detects hand landmarks in a sufficient percentage of frames
    \item \textbf{Coordinate Range Validation:} Confirmation that normalized coordinates fall within expected ranges $[0, 1]$
    \item \textbf{Sequence Length Verification:} Validation that all processed sequences conform to the target length $T = 30$
    \item \textbf{Class Label Consistency:} Verification of correct label assignment and encoding
\end{enumerate}

\subsubsection{Data Integrity Checks}

Additional integrity checks include:
\begin{itemize}
    \item Detection of missing or corrupted video files
    \item Validation of HDF5 file structure and accessibility
    \item Verification of train/validation/test split integrity
    \item Confirmation of feature vector dimensionality consistency
\end{itemize}

\section{Model Architecture and Training}

\subsection{LSTM-Based Sequence Classification Architecture}

\subsubsection{Network Design and Rationale}

The core of our sign language recognition system employs a Long Short-Term Memory (LSTM) neural network specifically designed for sequence classification. The architecture leverages the temporal modeling capabilities of LSTMs to learn patterns in hand keypoint sequences over time.

The complete model architecture consists of the following components:

\begin{enumerate}
    \item \textbf{Input Layer:} Accepts sequences of shape $(T, d_{feature}) = (30, 84)$
    \item \textbf{LSTM Layers:} Two-layer LSTM stack with hidden dimension $h = 256$
    \item \textbf{Classification Head:} Fully connected layer mapping to output classes
    \item \textbf{Output Layer:} Softmax activation for probability distribution over classes
\end{enumerate}

\subsubsection{Mathematical Formulation}

The LSTM-based classifier can be mathematically described as follows:

\textbf{Input Processing:}
\begin{equation}
\mathbf{X} \in \mathbb{R}^{T \times d_{feature}}
\end{equation}

\textbf{LSTM Forward Pass:}
For each time step $t \in \{1, 2, ..., T\}$, the LSTM computes:

\begin{align}
\mathbf{h}_t^{(1)}, \mathbf{c}_t^{(1)} &= \text{LSTM}_1(\mathbf{x}_t, \mathbf{h}_{t-1}^{(1)}, \mathbf{c}_{t-1}^{(1)}) \\
\mathbf{h}_t^{(2)}, \mathbf{c}_t^{(2)} &= \text{LSTM}_2(\mathbf{h}_t^{(1)}, \mathbf{h}_{t-1}^{(2)}, \mathbf{c}_{t-1}^{(2)})
\end{align}

\textbf{Classification:}
The final hidden state from the second LSTM layer is used for classification:

\begin{equation}
\mathbf{y} = \text{Softmax}(\mathbf{W}_c \mathbf{h}_T^{(2)} + \mathbf{b}_c)
\end{equation}

Where:
\begin{itemize}
    \item $\mathbf{W}_c \in \mathbb{R}^{C \times h}$ is the classification weight matrix
    \item $\mathbf{b}_c \in \mathbb{R}^C$ is the bias vector
    \item $C = 4$ is the number of output classes
    \item $h = 256$ is the hidden dimension
\end{itemize}

\subsubsection{Hyperparameter Configuration}

The model hyperparameters were selected based on empirical validation and computational constraints:

\begin{table}[H]
    \centering
    \caption{LSTM Model Hyperparameters}
    \label{tab:lstm_hyperparams}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        Input Size & 84 \\
        Hidden Size & 256 \\
        Number of LSTM Layers & 2 \\
        Dropout Rate & 0.3 \\
        Sequence Length & 30 \\
        Number of Classes & 4 \\
        Batch Size & 64 \\
        Learning Rate & 1×10⁻³ \\
        Weight Decay & 1×10⁻⁴ \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Training Methodology}

\subsubsection{Loss Function and Optimization}

The model is trained using the cross-entropy loss function, which is appropriate for multi-class classification tasks:

\begin{equation}
\mathcal{L}(\mathbf{y}, \hat{\mathbf{y}}) = -\sum_{i=1}^{C} y_i \log(\hat{y}_i)
\end{equation}

Where $\mathbf{y}$ is the one-hot encoded true label and $\hat{\mathbf{y}}$ is the predicted probability distribution.

The optimization process employs the AdamW optimizer, which combines the adaptive learning rate capabilities of Adam with improved weight decay regularization:

\begin{align}
\mathbf{m}_t &= \beta_1 \mathbf{m}_{t-1} + (1 - \beta_1) \nabla \mathcal{L}_t \\
\mathbf{v}_t &= \beta_2 \mathbf{v}_{t-1} + (1 - \beta_2) (\nabla \mathcal{L}_t)^2 \\
\hat{\mathbf{m}}_t &= \frac{\mathbf{m}_t}{1 - \beta_1^t} \\
\hat{\mathbf{v}}_t &= \frac{\mathbf{v}_t}{1 - \beta_2^t} \\
\boldsymbol{\theta}_{t+1} &= \boldsymbol{\theta}_t - \eta \left(\frac{\hat{\mathbf{m}}_t}{\sqrt{\hat{\mathbf{v}}_t} + \epsilon} + \lambda \boldsymbol{\theta}_t\right)
\end{align}

Where $\lambda = 1×10^{-4}$ is the weight decay parameter.

\subsubsection{Learning Rate Scheduling}

A ReduceLROnPlateau scheduler monitors validation F1-score and reduces the learning rate when improvement stagnates:

\begin{equation}
\eta_{new} = \eta_{current} \times \gamma
\end{equation}

Where $\gamma = 0.2$ is the reduction factor, applied when validation F1-score does not improve for 5 consecutive epochs.

\subsection{Training Infrastructure and Implementation}

\subsubsection{PyTorch Lightning Framework}

The training pipeline is implemented using PyTorch Lightning, which provides several advantages:

\begin{itemize}
    \item \textbf{Automation:} Automatic handling of training loops, validation, and logging
    \item \textbf{Reproducibility:} Consistent random seeding and deterministic operations
    \item \textbf{Scalability:} Easy GPU utilization and distributed training capabilities
    \item \textbf{Monitoring:} Built-in metric tracking and checkpoint management
\end{itemize}

\subsubsection{Training Configuration}

\begin{table}[H]
    \centering
    \caption{Training Configuration Details}
    \label{tab:training_config}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        Maximum Epochs & 80 \\
        Early Stopping Patience & 10 \\
        Validation Metric & F1-Score (Macro) \\
        Checkpoint Monitoring & Best Validation F1 \\
        Hardware Acceleration & GPU (when available) \\
        Precision & 32-bit floating point \\
        Gradient Clipping & None \\
        Deterministic Training & Enabled \\ \bottomrule
    \end{tabular}
\end{table}

\subsection{Evaluation Metrics}

\subsubsection{Classification Metrics}

The model performance is evaluated using standard classification metrics appropriate for multi-class problems:

\textbf{Accuracy:}
\begin{equation}
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\end{equation}

\textbf{Precision (per class):}
\begin{equation}
\text{Precision}_c = \frac{\text{TP}_c}{\text{TP}_c + \text{FP}_c}
\end{equation}

\textbf{Recall (per class):}
\begin{equation}
\text{Recall}_c = \frac{\text{TP}_c}{\text{TP}_c + \text{FN}_c}
\end{equation}

\textbf{F1-Score (per class):}
\begin{equation}
\text{F1}_c = 2 \times \frac{\text{Precision}_c \times \text{Recall}_c}{\text{Precision}_c + \text{Recall}_c}
\end{equation}

\textbf{Macro-averaged F1-Score:}
\begin{equation}
\text{F1}_{macro} = \frac{1}{C} \sum_{c=1}^{C} \text{F1}_c
\end{equation}

\subsubsection{Validation Strategy}

The validation strategy employs stratified sampling to maintain class distribution across data splits. Performance monitoring focuses on the macro-averaged F1-score to ensure balanced performance across all classes, particularly important given the relatively small dataset size.

\section{Implementation and Results}

\subsection{Experimental Setup and Training Results}

\subsubsection{Training Performance Analysis}

The LSTM-based sequence classifier was trained for a total of 23 epochs before early stopping was triggered. The training process demonstrated stable convergence with consistent improvement in both training and validation metrics.

\begin{table}[H]
    \centering
    \caption{Final Training Results}
    \label{tab:training_results}
    \begin{tabular}{@{}lcc@{}}
        \toprule
        \textbf{Metric} & \textbf{Training Set} & \textbf{Validation Set} \\ \midrule
        Loss & 0.18 & 0.29 \\
        Accuracy & 93\% & 87\% \\
        F1-Score (Macro) & 0.92 & 0.88 \\
        Best Epoch & \multicolumn{2}{c}{13} \\
        Total Epochs & \multicolumn{2}{c}{23} \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Learning Curve Analysis}

The training curves indicate effective learning without significant overfitting. The validation loss stabilized around epoch 13, leading to model checkpoint selection at this point. The early stopping mechanism prevented overfitting by halting training when validation F1-score ceased improving for 10 consecutive epochs.

\subsection{Test Set Evaluation}

\subsubsection{Overall Performance Metrics}

The final model evaluation on the held-out test set yielded the following results:

\begin{table}[H]
    \centering
    \caption{Test Set Performance Metrics}
    \label{tab:test_results}
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Metric} & \textbf{Value} \\ \midrule
        Test Accuracy & 0.800 \\
        Test Loss & 0.814 \\
        Macro F1-Score & 0.787 \\
        Weighted F1-Score & 0.794 \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Class-wise Performance Analysis}

Detailed per-class performance reveals varying recognition accuracy across different sign gestures:

\begin{table}[H]
    \centering
    \caption{Per-Class Classification Results}
    \label{tab:class_results}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \midrule
        clavier & 1.00 & 1.00 & 1.00 & 5 \\
        disque\_dur & 0.57 & 0.80 & 0.67 & 5 \\
        ordinateur & 0.83 & 1.00 & 0.91 & 5 \\
        souris & 1.00 & 0.40 & 0.57 & 5 \\ \midrule
        \textbf{Macro avg} & \textbf{0.85} & \textbf{0.80} & \textbf{0.79} & \textbf{20} \\
        \textbf{Weighted avg} & \textbf{0.85} & \textbf{0.80} & \textbf{0.79} & \textbf{20} \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Confusion Matrix Analysis}

The confusion matrix reveals specific patterns in classification errors:

\begin{itemize}
    \item \textbf{Perfect Classification:} 'clavier' achieved perfect precision and recall
    \item \textbf{High Accuracy:} 'ordinateur' demonstrated excellent recall with good precision
    \item \textbf{Confusion Patterns:} Primary confusion occurred between 'souris' and 'disque\_dur' classes
    \item \textbf{Class Imbalance Effects:} Small sample sizes (5 per class) amplify individual misclassification impacts
\end{itemize}

\subsection{Inference Performance and Real-time Capabilities}

\subsubsection{Processing Speed Analysis}

The complete inference pipeline, from video input to prediction output, demonstrates suitable performance for real-time applications:

\begin{table}[H]
    \centering
    \caption{Inference Performance Metrics}
    \label{tab:inference_performance}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Processing Stage} & \textbf{Average Time} \\ \midrule
        Video Frame Extraction & 0.12s per video \\
        Hand Detection (YOLOv8) & 0.03s per frame \\
        Keypoint Extraction (MediaPipe) & 0.02s per frame \\
        Sequence Classification (LSTM) & 0.01s per sequence \\
        \textbf{Total Pipeline} & \textbf{≈ 1.8s per video} \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Sample Inference Results}

Representative inference examples demonstrate the system's practical capabilities:

\textbf{Successful Recognition Example:}
\begin{itemize}
    \item Input Video: \texttt{souris46.mp4}
    \item Predicted Class: \texttt{souris}
    \item Confidence Score: 90.0\%
    \item Ground Truth: \texttt{souris}
    \item Result: Correct classification with high confidence
\end{itemize}

\subsection{System Architecture and Components}

\subsubsection{Complete Pipeline Overview}

The implemented system consists of several integrated components working in sequence:

\begin{enumerate}
    \item \textbf{Data Input Module:} Handles video file loading and format validation
    \item \textbf{Preprocessing Pipeline:} Executes hand detection and keypoint extraction
    \item \textbf{Feature Engineering:} Normalizes coordinates and constructs feature vectors
    \item \textbf{Sequence Processing:} Applies length standardization and temporal alignment
    \item \textbf{Model Inference:} Performs LSTM-based classification
    \item \textbf{Output Generation:} Produces predictions with confidence scores
\end{enumerate}

\subsubsection{Error Handling and Robustness}

The system incorporates several robustness measures:

\begin{itemize}
    \item \textbf{Missing Hand Detection:} Graceful handling of frames without detected hands
    \item \textbf{Variable Video Lengths:} Automatic padding/truncation for sequence length consistency
    \item \textbf{Quality Validation:} Input validation for video format and content quality
    \item \textbf{Fallback Mechanisms:} Default behaviors for edge cases and processing failures
\end{itemize}

\section{Model Training}
    % Definition: The process of training the sequence model.
    \subsection{Training Setup (PyTorch Lightning Trainer)}
        % Definition: Trainer configuration (epochs, accelerator, precision).
    \subsection{Callbacks Used}
        % Definition: ModelCheckpoint, EarlyStopping, LearningRateMonitor, CSVLogger.
    \subsection{Training Process and Observations}
        % (e.g., mention if early stopping was triggered, general convergence)

\section{Evaluation and Results}
    % Definition: Presentation and analysis of the model's performance.
    \subsection{Training and Validation Performance}
        \subsubsection{Loss and Metric Curves (Accuracy, F1-score)}
        \subsubsection{Best Model Selection}
    \subsection{Test Set Evaluation}
        \subsubsection{Overall Test Metrics (Accuracy, F1-score, Loss)}
        \subsubsection{Classification Report}
        \subsubsection{Confusion Matrix Analysis}
    \subsection{Inference on a New Video}
        \subsubsection{Inference Process}
        \subsubsection{Example Prediction and Visualization}

\section{Discussion and Analysis}

\subsection{Critical Analysis of Results}

The experimental results presented in this study demonstrate the effectiveness of the proposed keypoint-based approach for sign language recognition, while simultaneously revealing important insights about the current state of the field and the specific challenges encountered.

\subsubsection{Performance Interpretation}

The achieved test accuracy of 80\% with a macro F1-score of 0.787 represents a significant accomplishment given the constraints of the dataset and the complexity of the task. These results are particularly noteworthy when considered within the context of similar studies in the literature~\cite{Adaloglou21, Huang18}.

The class-wise performance analysis reveals interesting patterns:

\textbf{Superior Performance Classes:}
The "clavier" class achieved perfect classification (F1-score = 1.00), suggesting that this gesture exhibits highly distinctive temporal patterns that are easily distinguishable by the LSTM model. Similarly, "ordinateur" demonstrated excellent performance (F1-score = 0.91), indicating consistent execution patterns across different signers.

\textbf{Challenging Classifications:}
The confusion between "souris" and "disque\_dur" classes represents the primary source of classification errors. This confusion pattern suggests several possibilities:
\begin{enumerate}
    \item \textbf{Gestural Similarity:} The hand movements for these signs may share common kinematic patterns
    \item \textbf{Temporal Overlap:} The duration and rhythm of these gestures might be similar
    \item \textbf{Feature Insufficiency:} The current 84-dimensional feature space may not capture subtle distinguishing characteristics
\end{enumerate}

\subsubsection{Comparison with State-of-the-Art}

When positioned within the broader landscape of sign language recognition research, our results align favorably with reported accuracies in the literature. Studies using similar keypoint-based approaches have reported accuracies ranging from 75-90\% on small vocabulary tasks~\cite{Camgoz20, Koller20}. The 80\% accuracy achieved in this work falls within this range while utilizing a relatively compact and efficient model architecture.

\subsection{Technical Limitations and Constraints}

\subsubsection{Dataset-Related Limitations}

\textbf{Scale Constraints:}
The dataset size of 199 videos across 4 classes represents a significant limitation. Modern deep learning approaches typically require thousands to tens of thousands of samples per class for optimal performance. The current dataset size constrains the model's ability to:
\begin{itemize}
    \item Learn robust feature representations that generalize across signers
    \item Capture the natural variability in sign execution
    \item Resist overfitting to specific signing styles or recording conditions
\end{itemize}

\textbf{Diversity Limitations:}
The dataset lacks diversity in terms of:
\begin{itemize}
    \item Signer demographics (age, gender, signing experience)
    \item Recording conditions (lighting, backgrounds, camera angles)
    \item Sign execution variations (speed, amplitude, style)
\end{itemize}

\subsubsection{Methodological Limitations}

\textbf{Feature Representation Constraints:}
The current approach utilizes only hand keypoints, ignoring crucial elements of sign language communication:
\begin{itemize}
    \item Facial expressions and non-manual markers
    \item Upper body posture and shoulder movements
    \item Spatial relationships between hands and body
    \item Gaze direction and head orientation
\end{itemize}

\textbf{Temporal Modeling Limitations:}
The fixed sequence length of 30 frames may not optimally capture the natural temporal structure of signs:
\begin{itemize}
    \item Signs have variable natural durations
    \item Truncation may remove important gestural information
    \item Padding introduces artificial temporal patterns
\end{itemize}

\textbf{Model Architecture Constraints:}
While LSTM networks are effective for sequence modeling, they have inherent limitations:
\begin{itemize}
    \item Sequential processing prevents parallel computation
    \item Limited ability to model long-range dependencies compared to Transformer architectures
    \item Potential for vanishing gradients in very long sequences
\end{itemize}

\subsection{Broader Implications and Future Directions}

\subsubsection{Methodological Advances}

\textbf{Multi-Modal Feature Integration:}
Future work should explore the integration of multiple information sources:
\begin{equation}
\mathbf{F}_{multi} = [\mathbf{F}_{hands}, \mathbf{F}_{face}, \mathbf{F}_{pose}, \mathbf{F}_{context}]
\end{equation}

Where each component represents features from different modalities, potentially improving discrimination between similar signs.

\textbf{Advanced Sequence Modeling:}
The incorporation of attention mechanisms and Transformer architectures could address current temporal modeling limitations:
\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}

This would enable the model to focus on the most discriminative temporal segments of each gesture.

\textbf{Data Augmentation Strategies:}
Systematic data augmentation at the keypoint level could artificially expand the dataset:
\begin{itemize}
    \item Temporal augmentation (speed variation, temporal cropping)
    \item Spatial augmentation (rotation, scaling, translation)
    \item Noise injection for robustness
\end{itemize}

\subsubsection{Practical Deployment Considerations}

The current system demonstrates strong potential for real-world deployment, but several factors must be considered:

\textbf{Computational Efficiency:}
The keypoint-based approach offers significant computational advantages over raw video processing, with inference times suitable for real-time applications.

\textbf{Privacy and Accessibility:}
The offline processing capability addresses privacy concerns while ensuring accessibility across different computational environments.

\textbf{Scalability Challenges:}
Expanding to larger vocabularies will require addressing the dataset scaling challenges and potentially adopting transfer learning approaches.

\section{Conclusion and Future Perspectives}

\subsection{Summary of Contributions}

This research presents a comprehensive investigation into keypoint-based sign language recognition, making several notable contributions to the field:

\textbf{Technical Contributions:}
\begin{enumerate}
    \item Development of an end-to-end pipeline integrating state-of-the-art object detection (YOLOv8) with robust pose estimation (MediaPipe) for feature extraction
    \item Implementation of an efficient LSTM-based sequence classification architecture optimized for temporal gesture recognition
    \item Creation of a practical web-based deployment system enabling real-time sign language recognition
\end{enumerate}

\textbf{Methodological Contributions:}
\begin{enumerate}
    \item Demonstration of the effectiveness of keypoint-based approaches for reducing computational complexity while maintaining recognition accuracy
    \item Comprehensive evaluation framework incorporating multiple performance metrics and detailed error analysis
    \item Integration of modern deep learning frameworks (PyTorch Lightning) for reproducible research
\end{enumerate}

\textbf{Practical Contributions:}
\begin{enumerate}
    \item Development of a user-friendly web application demonstrating the practical applicability of the research
    \item Creation of a modular system architecture enabling easy extension to additional sign vocabularies
    \item Implementation of privacy-preserving offline processing capabilities
\end{enumerate}

\subsection{Key Achievements}

The project successfully achieved its primary objectives:

\begin{itemize}
    \item \textbf{High Recognition Accuracy:} Attained 80\% test accuracy with 0.787 macro F1-score on a multi-class sign language recognition task
    \item \textbf{Real-time Performance:} Demonstrated inference capabilities suitable for interactive applications (≈1.8 seconds per video)
    \item \textbf{Robust Pipeline:} Created a complete preprocessing pipeline capable of handling diverse video inputs
    \item \textbf{Scalable Architecture:} Developed a modular system design facilitating future extensions and improvements
\end{itemize}

\subsection{Research Impact and Significance}

This work contributes to the growing body of research in AI-assisted accessibility technology, specifically addressing the critical need for automated sign language recognition systems. The results demonstrate that efficient, keypoint-based approaches can achieve competitive performance while maintaining computational tractability for real-world deployment.

The integration of computer vision and sequence modeling techniques showcases the potential for interdisciplinary approaches to complex pattern recognition problems. The comprehensive evaluation and detailed analysis provide valuable insights for future researchers in the field.

\subsection{Future Research Directions}

Based on the findings and limitations identified in this work, several promising research directions emerge:

\subsubsection{Short-term Improvements}

\textbf{Dataset Enhancement:}
\begin{itemize}
    \item Expansion of the dataset to include larger vocabularies and more diverse signers
    \item Integration of publicly available sign language datasets for comparative evaluation
    \item Implementation of cross-lingual sign language recognition capabilities
\end{itemize}

\textbf{Model Architecture Refinements:}
\begin{itemize}
    \item Exploration of Transformer-based architectures for improved temporal modeling
    \item Investigation of attention mechanisms for identifying critical gesture components
    \item Implementation of ensemble methods combining multiple model architectures
\end{itemize}

\subsubsection{Long-term Research Goals}

\textbf{Multi-modal Integration:}
Future work should explore the incorporation of additional modalities:
\begin{itemize}
    \item Facial expression analysis for non-manual markers
    \item Upper body pose for contextual information
    \item Depth information for improved spatial understanding
\end{itemize}

\textbf{Continuous Sign Language Recognition:}
Extension from isolated sign recognition to continuous sign language understanding:
\begin{itemize}
    \item Development of segmentation algorithms for continuous signing
    \item Implementation of language models for sign sequence prediction
    \item Integration of grammatical and contextual constraints
\end{itemize}

\textbf{Personalization and Adaptation:}
\begin{itemize}
    \item Development of user-adaptive systems that learn individual signing styles
    \item Implementation of few-shot learning approaches for rapid vocabulary expansion
    \item Creation of personalized confidence calibration mechanisms
\end{itemize}

\subsection{Concluding Remarks}

This project demonstrates the viability and potential of keypoint-based approaches for sign language recognition, while highlighting the complex challenges that remain in creating truly comprehensive sign language understanding systems. The combination of modern computer vision techniques with classical sequence modeling approaches provides a strong foundation for future developments in this critical area of assistive technology.

The successful integration of research and practical deployment through the web application showcases the importance of bridging the gap between academic research and real-world applications. As the field continues to advance, the approaches and insights presented in this work will contribute to the development of more inclusive and accessible communication technologies.

The journey toward comprehensive sign language recognition remains ongoing, but this research provides a solid stepping stone toward that goal, offering both technical contributions and practical insights that will benefit future work in this important and impactful field.

\appendix
\section{Detailed Configuration Parameters}
    % Definition: A comprehensive list of all global parameters and their values used in the notebook.

% --- START OF SECTION 1 & 2 ---

\section{Introduction}
Video-based analysis is a rapidly advancing field within computer vision, with applications ranging from human-computer interaction and surveillance to automated assistance systems. A significant sub-domain is action and gesture recognition, which aims to interpret human movements from video sequences. This project focuses on a specific and highly impactful application: the classification of sign language.

\subsection{Background and Motivation}
Sign language is a visual language that uses hand gestures, facial expressions, and body language for communication. It is the primary means of communication for many deaf and hard-of-hearing communities worldwide. Automating the recognition of sign language can bridge communication gaps, create more inclusive digital tools, and offer new educational opportunities.

Traditional methods for action recognition often relied on complex feature engineering or computationally intensive 3D convolutional networks. A more modern and efficient approach, which this project adopts, is to first convert video frames into a compact and representative format—skeletal keypoints. By tracking the positions of key body joints (in this case, primarily the hands), the high-dimensional video data is transformed into a low-dimensional time-series representation. This sequence of keypoints can then be effectively processed by sequence models like Long Short-Term Memory (LSTM) networks, which are designed to understand temporal patterns.

\subsection{Problem Statement}
The core problem is to develop an automated system that can classify a given video of a person making a sign into one of several predefined sign language categories. The input to the system is a raw video file, and the output is the predicted class label for the sign performed in the video.

\subsection{Objectives}
The primary objectives of this project are:
\begin{enumerate}
    \item To implement a robust preprocessing pipeline that automatically detects the subject and extracts relevant hand keypoints from each video frame.
    \item To structure the extracted keypoint data into fixed-length sequences suitable for training a deep learning model.
    \item To build and train a sequence classification model, specifically an LSTM network, to learn the temporal dynamics of different signs.
    \item To evaluate the model's performance on a held-out test set using standard classification metrics.
    \item To demonstrate the system's end-to-end functionality by performing inference on a new, unseen video.
\end{enumerate}

\subsection{Pipeline Overview}
The project follows a modular pipeline, illustrated below:
\begin{enumerate}[label=\textbf{Step \arabic*:}]
    \item \textbf{ROI Detection:} For each frame in an input video, the YOLOv8 object detection model is used to identify a bounding box around the person. This step focuses the subsequent analysis on the relevant region.
    \item \textbf{Keypoint Extraction:} Within the detected ROI (or the full frame if no person is found), the MediaPipe Hands solution is applied to extract the (x, y) coordinates of 21 keypoints for up to two hands.
    \item \textbf{Data Structuring:} The extracted keypoints are normalized, concatenated into a fixed-size feature vector (84 dimensions), and sequenced. Videos are padded or truncated to a fixed length of 30 frames. The final dataset is stored in an HDF5 file.
    \item \textbf{Model Training:} An LSTM-based classifier is trained on the sequences of keypoint feature vectors to learn to distinguish between the different sign classes.
    \item \textbf{Evaluation:} The trained model's performance is assessed using metrics like accuracy, precision, recall, and the F1-score on the test dataset.
\end{enumerate}

\section{Environment Setup and Configuration}
This section details the software environment and the key parameters that govern the behavior of the entire pipeline.

\subsection{Required Libraries}
The project is implemented in Python and relies on a number of open-source libraries. The primary dependencies are installed via pip:
\begin{lstlisting}[language=bash]
%pip install ultralytics mediapipe opencv-python numpy torch
%pip install torchvision torchaudio tqdm scikit-learn matplotlib
%pip install h5py lightning pandas ipywidgets
\end{lstlisting}
The key libraries and their roles are:
\begin{itemize}
    \item \textbf{ultralytics:} Provides the YOLOv8 model for object detection.
    \item \textbf{mediapipe:} Used for robust hand keypoint extraction.
    \item \textbf{opencv-python:} Handles all video and image processing tasks (reading frames, drawing annotations).
    \item \textbf{torch \& lightning:} The core deep learning framework (PyTorch) and its high-level wrapper (PyTorch Lightning) for defining, training, and evaluating the model.
    \item \textbf{h5py:} For efficient storage and retrieval of the large keypoint dataset.
    \item \textbf{scikit-learn \& pandas:} Used for data splitting and evaluation metrics (classification report, confusion matrix).
\end{itemize}

\subsection{Global Configuration Parameters}
A set of global parameters was defined to ensure reproducibility and allow for easy modification of the pipeline's behavior. A random seed of 42 was used throughout the project.

\subsubsection{Preprocessing Parameters}
\begin{table}[H]
    \centering
    \caption{Key Preprocessing and Data Parameters}
    \label{tab:preprocessing_params}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        YOLO Model & \texttt{yolov8n.pt} \\
        YOLO Confidence Threshold & 0.4 \\
        MediaPipe Max Hands & 2 \\
        MediaPipe Detection Confidence & 0.5 \\
        Number of Frames per Video & 30 \\
        Features per Frame (Input Size) & 84 \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Model and Training Parameters}
\begin{table}[H]
    \centering
    \caption{LSTM Model and Training Hyperparameters}
    \label{tab:training_params}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        Model Type & LSTM \\
        LSTM Hidden Size & 256 \\
        LSTM Layers & 2 \\
        LSTM Dropout & 0.3 \\
        Batch Size & 64 \\
        Learning Rate & 1e-3 \\
        Weight Decay & 1e-4 \\
        Max Epochs & 80 \\
        Early Stopping Patience & 10 \\ \bottomrule
    \end{tabular}
\end{table}

% --- END OF SECTION 1 & 2 ---

% --- START OF SECTION 3 ---

\section{Data Acquisition and Preprocessing}
The foundation of any machine learning model is its data. This section describes the dataset used for training and the multi-step preprocessing pipeline designed to convert raw video files into structured, numerical sequences of keypoints.

\subsection{Dataset Description}
The model was trained on a custom video dataset located in the \texttt{./SignLanguageDataset} directory.

\subsubsection{Dataset Structure}
The dataset is organized in a common format for classification tasks, where each class has its own subdirectory containing the corresponding video samples. The structure is as follows:
\begin{verbatim}
SignLanguageDataset/
|-- class_1_name/ (e.g., clavier)
|   |-- video1.mp4
|   |-- video2.mp4
|   `-- ...
|-- class_2_name/ (e.g., disque_dur)
|   |-- video3.mp4
|   `-- ...
`-- ...
\end{verbatim}

\subsubsection{Class Labels and Distribution}
The notebook automatically discovers the classes by reading the subdirectory names. For the execution detailed in this report, the following four classes were found and mapped to integer labels:
\begin{itemize}
    \item \texttt{clavier}: 0
    \item \texttt{disque\_dur}: 1
    \item \texttt{ordinateur}: 2
    \item \texttt{souris}: 3
\end{itemize}
A total of \textbf{199 videos} were processed across these four classes.

\subsection{Keypoint Extraction Pipeline}
The core of the preprocessing stage is to transform each video frame into a meaningful, low-dimensional feature vector. This is achieved through a two-step process involving object detection and pose estimation.

\subsubsection{YOLOv8 for Region of Interest (ROI) Detection}
To improve the accuracy and efficiency of hand detection, a pre-trained YOLOv8 model (\texttt{yolov8n.pt}) is first used to detect persons in the frame. The bounding box of the detected person is then used as a Region of Interest (ROI). This ensures that MediaPipe focuses its search for hands within the most relevant area of the image, reducing the likelihood of false positives from background objects. A confidence threshold of 0.4 is used to filter out weak detections.

\subsubsection{MediaPipe for Hand Keypoint Extraction}
Once an ROI is established, the MediaPipe Hands solution is applied to the cropped image region. MediaPipe is a powerful framework from Google that provides fast and accurate pose estimation. For this project, it is configured to detect a maximum of two hands. For each detected hand, it returns the 2D coordinates of 21 key landmarks (e.g., wrist, fingertips, knuckles).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{mediapipe_hand_landmarks.png} % Placeholder for a hand landmark diagram
    \caption{The 21 hand landmarks detected by MediaPipe.}
    \label{fig:mediapipe_hands}
\end{figure}

\subsubsection{Keypoint Normalization and Feature Vector Creation}
The keypoints extracted by MediaPipe are initially relative to the ROI. A crucial step is to convert these back to coordinates relative to the full frame dimensions. This ensures that the keypoint locations are consistent regardless of the person's position or scale in the video.

The process for a single frame is as follows:
\begin{enumerate}
    \item A zero-vector of size 84 is initialized to hold the data for two hands (2 hands $\times$ 21 keypoints/hand $\times$ 2 coordinates/keypoint).
    \item For each detected hand (up to two), its 21 (x, y) keypoints are normalized by the full frame's width and height, respectively.
    \item These 42 values are flattened into a single vector.
    \item The keypoints for the left and right hands are placed into their designated slots in the 84-dimensional feature vector. If a hand is not detected, its corresponding slot remains filled with zeros.
\end{enumerate}
This results in a consistent feature vector of size 84 for every frame of every video.

\subsection{Sequence Generation and Storage}
\subsubsection{Fixed-Length Sequence Creation (Padding/Truncation)}
Sequence models like LSTMs require inputs of a fixed length. As the source videos have varying durations, a standardized sequence length of \textbf{30 frames} was chosen.
\begin{itemize}
    \item \textbf{If a video has more than 30 frames}, a central crop of 30 frames is taken to capture the most representative part of the action.
    \item \textbf{If a video has fewer than 30 frames}, the sequence is padded with vectors of zeros until it reaches the required length of 30.
\end{itemize}
This process ensures that every sample fed to the model has the shape (30, 84).

\subsubsection{HDF5 Dataset Format}
To handle the data efficiently, all processed sequences are stored in a single HDF5 (Hierarchical Data Format 5) file named \texttt{model.h5}. HDF5 is ideal for storing large numerical datasets as it allows for fast, chunked access without loading the entire file into memory. Each processed video sequence is saved as a dataset within the HDF5 file, with its corresponding class label and original file path stored as attributes.

% --- END OF SECTION 3 ---

% --- START OF SECTION 4 ---

\section{Dataset and DataLoader Implementation}
With the data preprocessed and stored in an HDF5 file, the next step is to create an efficient pipeline for loading this data during model training and evaluation. This is accomplished using a custom PyTorch \texttt{Dataset} and a PyTorch Lightning \texttt{DataModule}.

\subsection{Custom Dataset Class (\texttt{KeypointSequenceDataset})}
A custom class, \texttt{KeypointSequenceDataset}, was created to interface with the HDF5 file. This class inherits from PyTorch's \texttt{Dataset} and implements the necessary methods for data loading:
\begin{itemize}
    \item \texttt{\_\_init\_\_()}: Initializes the dataset by taking the path to the HDF5 file and a list of keys (representing the video samples to be included in this specific dataset split, e.g., train, val, or test).
    \item \texttt{\_\_len\_\_()}: Returns the total number of samples in the dataset split.
    \item \texttt{\_\_getitem\_\_()}: The core method for data retrieval. Given an index, it opens the HDF5 file, accesses the corresponding sequence data and its label attribute, converts them to PyTorch tensors, and returns them as a tuple `(sequence, label)`.
\end{itemize}
This approach is memory-efficient because it only loads one sample at a time from the disk, rather than loading the entire dataset into RAM.

\subsection{PyTorch Lightning DataModule (\texttt{KeypointDataModule})}
To encapsulate all data-related logic—from splitting to creating loaders—a \texttt{KeypointDataModule} was implemented using PyTorch Lightning. This class centralizes data handling and keeps it separate from the model definition.

\subsubsection{Initialization and Data Splitting}
The \texttt{DataModule} is initialized with the HDF5 file path, batch size, and split ratios. The main logic resides in the \texttt{setup()} method:
\begin{enumerate}
    \item It opens the HDF5 file to read the list of all available dataset keys (video samples) and the label map (class-to-index mapping).
    \item It uses \texttt{scikit-learn}'s \texttt{train\_test\_split} function to partition the dataset keys. The split is performed in a stratified manner to ensure that the class distribution is preserved across the train, validation, and test sets.
    \item The dataset was split as follows:
        \begin{itemize}
            \item \textbf{Training Set:} 70\% of the data (139 samples)
            \item \textbf{Validation Set:} 20\% of the data (40 samples)
            \item \textbf{Test Set:} 10\% of the data (20 samples)
        \end{itemize}
\end{enumerate}

\subsubsection{DataLoaders}
The \texttt{DataModule} defines methods to return PyTorch \texttt{DataLoader} instances for each split. These loaders are responsible for:
\begin{itemize}
    \item Batching the data according to the specified \texttt{BATCH\_SIZE} (64).
    \item Shuffling the training data at the beginning of each epoch to reduce variance and improve model generalization.
    \item Managing parallel data loading with multiple workers (though set to 0 in this notebook for simplicity and compatibility).
\end{itemize}
The methods \texttt{train\_dataloader()}, \texttt{val\_dataloader()}, and \texttt{test\_dataloader()} are automatically called by the PyTorch Lightning Trainer at the appropriate stages of the training and evaluation loop. This setup provides a clean and organized way to feed data to the model.

% --- END OF SECTION 4 ---

% --- START OF SECTION 5 ---

\section{Sequence Model Architecture}
To classify the temporal sequences of keypoints, a recurrent neural network (RNN) architecture is employed. Specifically, a Long Short-Term Memory (LSTM) network was chosen for its proven ability to capture long-range dependencies in sequential data, which is essential for understanding the dynamic nature of sign language gestures. The model is implemented as a PyTorch Lightning module, which encapsulates the network definition, the training step logic, and the optimizer configuration.

\subsection{LSTM-based Classifier (\texttt{LSTMClassifier})}
The \texttt{LSTMClassifier} class defines the neural network architecture. It is composed of two main components: an LSTM stack and a final classification layer.

\subsubsection{LSTM Network}
The core of the model is a multi-layered LSTM network. LSTMs are a type of RNN that use a system of gates (input, forget, and output) to regulate the flow of information. This allows the network to selectively remember or forget information over long sequences, mitigating the vanishing gradient problem that affects simple RNNs.

The LSTM layer is configured with the following key hyperparameters:
\begin{itemize}
    \item \textbf{input\_size:} 84. This corresponds to the number of features in each frame of the sequence (2 hands $\times$ 21 keypoints $\times$ 2 coordinates).
    \item \textbf{hidden\_size:} 256. This defines the number of features in the hidden state of the LSTM, representing the dimensionality of the learned sequence representation.
    \item \textbf{num\_layers:} 2. A stacked LSTM with two layers is used. This allows the model to learn hierarchical features, where the first layer captures lower-level temporal patterns and the second layer learns more abstract, higher-level patterns from the output of the first.
    \item \textbf{batch\_first:} True. This specifies that the input tensors will have the batch dimension as the first dimension (shape: `[batch_size, sequence_length, input_size]`).
    \item \textbf{dropout:} 0.3. Dropout is applied between the LSTM layers during training to prevent overfitting by randomly setting a fraction of neuron activations to zero.
\end{itemize}

\subsubsection{Classification Head}
The output of the LSTM network is the sequence of hidden states for each time step. For classification, we are interested in the final representation of the entire sequence. Therefore, only the hidden state of the last time step from the final LSTM layer, $h_n$, is used. This vector is then passed through a single fully connected (linear) layer, which maps the 256-dimensional hidden state to the number of output classes (4 in this case). A softmax activation is implicitly applied by the loss function during training.

The forward pass of the model can be summarized as:
\begin{equation}
    \text{output} = \text{Linear}(\text{LSTM}(x)_{-1})
\end{equation}
where $x$ is the input sequence and $(\cdot)_{-1}$ denotes the final hidden state.

\subsection{Loss Function and Optimizer}
\begin{itemize}
    \item \textbf{Loss Function:} \texttt{nn.CrossEntropyLoss}. This is the standard loss function for multi-class classification tasks. It combines a LogSoftmax layer and a Negative Log-Likelihood loss, making it suitable for training classification models.
    \item \textbf{Optimizer:} \texttt{torch.optim.AdamW}. The AdamW optimizer is an extension of the Adam optimizer that improves weight decay regularization. It is a robust and widely used choice for training deep neural networks, known for its adaptive learning rate capabilities. The learning rate was set to \texttt{1e-3} and weight decay to \texttt{1e-4}.
    \item \textbf{Learning Rate Scheduler:} \texttt{ReduceLROnPlateau}. To dynamically adjust the learning rate during training, a scheduler is used. It monitors the validation F1-score (\texttt{val\_f1}) and reduces the learning rate by a factor of 0.2 if the metric does not improve for a set number of epochs (patience of 5).
\end{itemize}

\subsection{Evaluation Metrics}
In addition to the loss, the model's performance is tracked using standard classification metrics, implemented via the \texttt{torchmetrics} library:
\begin{itemize}
    \item \textbf{Accuracy:} The proportion of correctly classified sequences.
    \item \textbf{F1-Score (Macro):} The macro-averaged F1-score, which is the harmonic mean of precision and recall. It is a useful metric for evaluating performance on imbalanced datasets, as it calculates the metric independently for each class and then takes the unweighted average.
\end{itemize}
These metrics are calculated and logged for the training, validation, and test sets.

% --- END OF SECTION 5 ---

% --- START OF SECTION 6 ---

\section{Model Training}
The training process is orchestrated by the PyTorch Lightning \texttt{Trainer}, which automates the training loop, validation, testing, and hardware management. This approach simplifies the code by abstracting away boilerplate engineering code, allowing for a clearer focus on the model and data logic.

\subsection{Training Setup (PyTorch Lightning Trainer)}
The \texttt{Trainer} object was configured with several key arguments to control the training environment and procedure:
\begin{itemize}
    \item \textbf{max\_epochs:} 80. The training was set to run for a maximum of 80 epochs. However, due to early stopping, the actual number of epochs was lower.
    \item \textbf{accelerator:} \texttt{"gpu"} if available, otherwise \texttt{"cpu"}. The notebook automatically utilizes a CUDA-enabled GPU for faster training when one is detected. The reported training run was performed on a GPU.
    \item \textbf{devices:} 1. A single GPU or CPU was used.
    \item \textbf{logger:} \texttt{CSVLogger}. A logger was configured to save all training and validation metrics (e.g., loss, accuracy, F1-score) to a \texttt{metrics.csv} file within the \texttt{model\_yolo/action\_rec\_logs} directory. This enables post-training analysis and visualization of the learning curves.
    \item \textbf{deterministic:} True. This flag, combined with setting random seeds, aims to make the training process as reproducible as possible.
\end{itemize}

\subsection{Callbacks Used}
Callbacks are special objects that can execute custom code at various stages of the training loop. The following callbacks were used to manage the training process:
\begin{itemize}
    \item \textbf{ModelCheckpoint:} This callback monitors a specific metric during validation and saves a checkpoint of the model when that metric improves.
        \begin{itemize}
            \item It was configured to monitor the \texttt{val\_f1} score.
            \item The \texttt{mode} was set to \texttt{'max'}, so it saved the model with the highest validation F1-score.
            \item Only the single best model (\texttt{save\_top\_k=1}) was retained.
        \end{itemize}
    \item \textbf{EarlyStopping:} This callback halts the training process if a monitored metric stops improving.
        \begin{itemize}
            \item It also monitored the \texttt{val\_f1} score in \texttt{'max'} mode.
            \item The \textbf{patience} was set to 10 epochs. This means training would stop if the validation F1-score did not improve for 10 consecutive validation cycles.
        \end{itemize}
    \item \textbf{LearningRateMonitor:} This callback logs the learning rate at each epoch, which is useful for observing the behavior of the learning rate scheduler.
\end{itemize}

\subsection{Training Process and Observations}
The training was initiated by calling the \texttt{trainer.fit(model, datamodule=data\_module)} method. The trainer automatically handled the iteration over epochs and batches, the forward and backward passes, optimizer steps, and metric logging.

Based on the notebook output, the following observations were made:
\begin{itemize}
    \item The training process successfully utilized the available GPU.
    \item The \textbf{EarlyStopping} callback was triggered after epoch 23, as the validation F1-score did not show improvement for 10 epochs. This prevented the model from overfitting and saved unnecessary computation time.
    \item The \textbf{ModelCheckpoint} callback saved the best performing model from \textbf{epoch 13}, which achieved a validation F1-score of approximately 0.88. This checkpoint was used for all subsequent evaluation and inference tasks.
\end{itemize}
The use of these automated tools from PyTorch Lightning ensured a robust and efficient training loop.

% --- END OF SECTION 6 ---

% --- START OF SECTION 7 ---

\section{Evaluation and Results}
After training, the performance of the model was thoroughly evaluated using both the validation data (for model selection) and the unseen test data (for final performance reporting). This section presents the key results, including learning curves, classification metrics, and an example of real-world inference.

\subsection{Training and Validation Performance}

\subsubsection{Loss and Metric Curves (Accuracy, F1-score)}
The learning progress was tracked using the \texttt{CSVLogger}. Figure \ref{fig:training_curves} displays the training and validation loss, accuracy, and F1-score across the training epochs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{training_curves.png} % <-- IMPORTANT: Replace with the actual image file from your notebook output
    \caption{Training \& Validation Curves: (Left) Loss vs. Epoch, (Right) Accuracy \& F1-Score vs. Epoch.}
    \label{fig:training_curves}
\end{figure}

The curves show typical learning behavior:
\begin{itemize}
    \item The training loss consistently decreases, while the validation loss plateaus, indicating that the model has learned the patterns in the data and is beginning to reach its generalization limit.
    \item Both training and validation accuracy/F1-score increase steadily before the validation metrics plateau. This confirms that the model is effectively learning to distinguish between the classes.
\end{itemize}

\subsubsection{Best Model Selection}
The \texttt{ModelCheckpoint} callback saved the model that achieved the highest validation F1-score. According to the training logs, this occurred at \textbf{epoch 13}, with a \texttt{val\_f1} score of approximately \textbf{0.88}. This specific checkpoint was used for the final evaluation on the test set to ensure an unbiased assessment of the model's generalization capability. Training was halted by the \texttt{EarlyStopping} callback at epoch 23.

\subsection{Test Set Evaluation}
The best model was loaded from the checkpoint and evaluated on the held-out test set, which it had not seen during training or model selection.

\subsubsection{Overall Test Metrics}
The final performance metrics on the test set were as follows:
\begin{table}[H]
    \centering
    \caption{Final Test Set Performance Metrics}
    \label{tab:test_metrics}
    \begin{tabular}{@{}lc@{}}
        \toprule
        \textbf{Metric} & \textbf{Value} \\ \midrule
        Test Loss & 0.814 \\
        Test Accuracy & 0.800 \\
        Test F1-Score (Macro) & 0.787 \\ \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Classification Report}
A more detailed breakdown of performance for each class is provided by the classification report (Table \ref{tab:class_report}).

\begin{table}[H]
    \centering
    \caption{Classification Report on the Test Set}
    \label{tab:class_report}
\begin{tabular}{lrrrr}
\toprule
{} &  precision &  recall &  f1-score &  support \\
\midrule
\textbf{clavier   } &       1.00 &    1.00 &      1.00 &        5 \\
\textbf{disque\_dur} &       0.57 &    0.80 &      0.67 &        5 \\
\textbf{ordinateur} &       0.83 &    1.00 &      0.91 &        5 \\
\textbf{souris    } &       1.00 &    0.40 &      0.57 &        5 \\
\midrule
\textbf{accuracy  } &            &         &      0.80 &       20 \\
\textbf{macro avg } &       0.85 &    0.80 &      0.79 &       20 \\
\textbf{weighted avg} &       0.85 &    0.80 &      0.79 &       20 \\
\bottomrule
\end{tabular}
\end{table}

The report shows strong performance for the 'clavier' and 'ordinateur' classes. The 'disque\_dur' class has high recall but lower precision, while the 'souris' class has perfect precision but lower recall, indicating it was sometimes confused with other classes.

\subsubsection{Confusion Matrix Analysis}
The confusion matrix (Figure \ref{fig:confusion_matrix}) visually confirms the findings from the classification report.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{confusion_matrix.png} % <-- IMPORTANT: Replace with the actual image file from your notebook output
    \caption{Confusion Matrix on the Test Set.}
    \label{fig:confusion_matrix}
\end{figure}

The matrix clearly shows the misclassifications. For example, three instances of 'souris' were incorrectly predicted as 'disque\_dur', which explains the lower recall for 'souris' and lower precision for 'disque\_dur'.

\subsection{Inference on a New Video}
To demonstrate the practical application of the trained model, an inference test was conducted on a randomly selected video from the dataset that was not part of the training or validation splits.
\begin{itemize}
    \item \textbf{Sample Video:} \texttt{.../souris/souris46.mp4}
    \item \textbf{Predicted Class:} \texttt{souris}
    \item \textbf{Confidence Score:} 90.0\%
\end{itemize}
The model correctly identified the class of the sign with high confidence. The notebook output included visualizations of the annotated video frames, with the detected keypoints and the predicted class overlaid, confirming the pipeline's ability to process raw video and produce an accurate classification.

% --- END OF SECTION 7 ---

% --- START OF SECTIONS 8, 9, and Appendix ---

\section{Discussion}
The results presented in the previous section demonstrate the viability of the implemented pipeline. This section provides a deeper analysis of these results, discusses the inherent limitations of the current approach, and proposes avenues for future enhancements.

\subsection{Analysis of Results}
The model achieved a respectable test accuracy of 80\% and a macro F1-score of 0.79. This indicates that the core methodology—using YOLOv8 and MediaPipe to generate keypoint sequences for an LSTM classifier—is fundamentally sound.

\begin{itemize}
    \item \textbf{Strong Performance on Distinct Signs:} The model performed exceptionally well on the 'clavier' and 'ordinateur' classes, achieving perfect recall and high precision. This suggests that the signs for these words have distinct and consistent temporal patterns that the LSTM was able to learn effectively.

    \item \textbf{Class Confusion:} The main source of error, as highlighted by the confusion matrix (Figure \ref{fig:confusion_matrix}), was the misclassification between 'souris' and 'disque\_dur'. Three 'souris' samples were incorrectly classified as 'disque\_dur'. This could be due to several factors:
        \begin{itemize}
            \item The hand motions for these two signs might be inherently similar.
            \item The dataset might contain ambiguous or poorly executed examples for these classes.
            \item The model may not have had enough data to learn the subtle distinguishing features between them.
        \end{itemize}

    \item \textbf{YOLO+MediaPipe Effectiveness:} The two-stage preprocessing pipeline proved effective. Using YOLOv8 to first locate the person provides a robust method for establishing a region of interest, which likely improves the speed and accuracy of the subsequent MediaPipe hand detection.
\end{itemize}

\subsection{Limitations of the Approach}
Despite its success, the current system has several limitations:
\begin{itemize}
    \item \textbf{Dataset Size:} The dataset of 199 videos across 4 classes is relatively small for a deep learning task. This limits the model's ability to generalize and may be a primary reason for the observed class confusion.
    \item \textbf{Lack of Facial and Body Keypoints:} True sign language recognition relies not only on hand gestures but also on facial expressions and upper body posture. The current model only uses hand keypoints, ignoring this rich contextual information.
    \item \textbf{Fixed Number of Hands:} The model is hard-coded to handle a maximum of two hands. It does not account for one-handed signs versus two-handed signs in a structural way.
    \item \textbf{Viewpoint and Occlusion Sensitivity:} The performance of both YOLOv8 and MediaPipe can degrade with unusual camera angles, poor lighting, or if the hands are occluded by objects or other body parts.
    \item \textbf{Static Background Assumption:} While not explicitly assumed, the model might struggle in dynamic environments with significant background motion or multiple people, which could confuse the initial YOLO detection.
\end{itemize}

\subsection{Potential Future Work and Improvements}
Building on the current foundation, several enhancements could be explored:
\begin{itemize}
    \item \textbf{Expand the Dataset:} The most impactful improvement would be to train the model on a much larger and more diverse dataset, covering more signs and including more examples per sign from various signers.
    \item \textbf{Incorporate More Keypoints:} Extend the feature vector to include keypoints from the face (using MediaPipe Face Mesh) and upper body/shoulders (using MediaPipe Pose) to provide the model with more contextual information.
    \item \textbf{Explore Advanced Models:} While LSTM is effective, more advanced architectures like Gated Recurrent Units (GRUs), Transformers, or Temporal Convolutional Networks (TCNs) could be implemented and compared for their ability to model temporal sequences.
    \item \textbf{Data Augmentation:} Apply data augmentation techniques at the keypoint level, such as adding noise, scaling, or slight temporal shifting, to create a more robust model that is less sensitive to minor variations in sign execution.
    \item \textbf{Real-Time Inference:} Adapt the pipeline for real-time (webcam) inference by optimizing the frame processing loop.
\end{itemize}


\section{Conclusion}

\subsection{Summary of Findings}
This project successfully designed and implemented an end-to-end pipeline for sign language classification from video. The system effectively integrates YOLOv8 for person detection, MediaPipe for hand keypoint extraction, and an LSTM network for sequence classification. The model achieved an overall test accuracy of 80\% and a macro F1-score of 0.79 on a custom 4-class dataset. The evaluation highlighted the model's strengths in recognizing distinct gestures and identified specific areas of class confusion that could be addressed with a larger dataset.

\subsection{Overall Assessment of the Pipeline}
The chosen approach of converting video to keypoint sequences before classification proves to be a highly efficient and effective strategy. It significantly reduces the computational complexity compared to methods that operate directly on raw video pixels (e.g., 3D CNNs) while retaining the essential spatio-temporal information required for the task. The use of PyTorch Lightning streamlined the development and training process, enabling robust experimentation and evaluation.

In conclusion, this project serves as a strong proof-of-concept and a solid foundation for building more advanced and comprehensive sign language recognition systems. The modular design allows for individual components to be upgraded (e.g., swapping the LSTM for a Transformer) or expanded (e.g., adding more keypoint types) in future iterations.

\appendix
\section{Detailed Configuration Parameters}
For reproducibility, the complete list of global parameters used in the notebook is provided below.
\begin{table}[H]
    \centering
    \caption{Full Configuration Parameter List}
    \label{tab:full_config}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\ \midrule
        \multicolumn{2}{c}{\textbf{Paths}} \\
        RAW\_VIDEO\_DATA\_DIR & \texttt{./SignLanguageDataset} \\
        PROCESSED\_DATA\_DIR & \texttt{./data-yolo} \\
        H5\_FILENAME & \texttt{model.h5} \\
        MODEL\_SAVE\_DIR & \texttt{./model\_yolo} \\
        \midrule
        \multicolumn{2}{c}{\textbf{Preprocessing}} \\
        YOLO\_MODEL\_NAME & \texttt{yolov8n.pt} \\
        YOLO\_CONF\_THRESHOLD & 0.4 \\
        MAX\_NUM\_HANDS\_MEDIAPIPE & 2 \\
        MIN\_DETECTION\_CONF\_MEDIAPIPE & 0.5 \\
        MIN\_TRACKING\_CONF\_MEDIAPIPE & 0.5 \\
        NUM\_FRAMES\_PER\_VIDEO & 30 \\
        \midrule
        \multicolumn{2}{c}{\textbf{Model \& Training}} \\
        SEQUENCE\_MODEL\_TYPE & LSTM \\
        INPUT\_SIZE & 84 \\
        HIDDEN\_SIZE\_LSTM & 256 \\
        NUM\_LAYERS\_LSTM & 2 \\
        DROPOUT\_LSTM & 0.3 \\
        BATCH\_SIZE & 64 \\
        LEARNING\_RATE & 1e-3 \\
        WEIGHT\_DECAY & 1e-4 \\
        EPOCHS & 80 \\
        PATIENCE\_EARLY\_STOPPING & 10 \\
        RANDOM\_SEED & 42 \\ \bottomrule
    \end{tabular}
\end{table}

% --- END OF SECTIONS 8, 9, and Appendix ---

% --- WEB APPLICATION SECTION ---

\section{Web Application: Real-Time Sign Language Recognition}
To make the results of this project accessible and interactive, a modern web application was developed. This application allows users to upload or record videos directly from their browser and receive instant predictions of sign language gestures using the trained LSTM model.

\subsection{Features}
\begin{itemize}
    \item \textbf{Video Upload:} Users can upload video files (MP4, AVI, MOV) for recognition.
    \item \textbf{Live Recording:} The app supports recording videos directly from a webcam.
    \item \textbf{AI Recognition:} The backend processes the video, extracts hand keypoints, and predicts the sign using the trained model.
    \item \textbf{Confidence Scores:} The app displays the predicted class and the confidence for each possible class.
    \item \textbf{Modern UI:} The interface is responsive, user-friendly, and works on both desktop and mobile devices.
\end{itemize}

\subsection{Technical Integration}
The web app is built with Flask (\texttt{app.py}) and uses HTML5/JavaScript for the frontend (\texttt{templates/index.html}). The backend loads the YOLOv8 and LSTM models, processes uploaded or recorded videos frame by frame, and returns predictions to the user in real time. The startup script (\texttt{run_app.py}) checks for all required files and launches the server.

\subsection{Usage Guide}
\begin{enumerate}
    \item Ensure the following files exist: \texttt{yolov8n.pt}, \texttt{data-yolo/model.h5}, and at least one model checkpoint in \texttt{model\_yolo/}.
    \item Install dependencies: \texttt{pip install -r requirements.txt}
    \item Start the app: \texttt{python run\_app.py}
    \item Open a browser and go to \texttt{http://localhost:5000}
    \item Upload or record a video and view the prediction results.
\end{enumerate}

\subsection{Extending the System}
To add new sign classes:
\begin{itemize}
    \item Add new video samples to \texttt{SignLanguageDataset/} in a new subfolder for each class.
    \item Retrain the model using the provided notebook (\texttt{model - FINAL .ipynb}).
    \item Place the new model checkpoint in \texttt{model\_yolo/} and restart the web app.
\end{itemize}

\subsection{Project Structure Overview}
\begin{verbatim}
sign-lang-classification/
├── app.py                 # Flask backend
├── run_app.py             # Startup script
├── requirements.txt       # Python dependencies
├── templates/
│   └── index.html         # Web interface
├── uploads/               # Temporary upload directory
├── model_yolo/            # Trained model checkpoints
├── data-yolo/             # Processed dataset
├── yolov8n.pt             # YOLO model
└── SignLanguageDataset/   # Raw video dataset
\end{verbatim}

This web application demonstrates the practical impact of the project, making advanced sign language recognition accessible to end users in an intuitive and interactive way.

% --- END WEB APPLICATION SECTION ---

\bibliography{references}
\bibliographystyle{ieeetr}

% Bibliography entries (since we don't have a separate .bib file, we'll include them directly)
\begin{thebibliography}{99}

\bibitem{Adaloglou21}
N. Adaloglou, T. Chatzis, A. Papastratis, A. Stergioulas, G. Papadopoulos, O. Zagoris, S. Pratsinakis, T. Theodoropoulos, W. Kelleher, and F. Ljungberg, 
``A comprehensive study on deep learning-based methods for sign language recognition,''
\textit{IEEE Transactions on Multimedia}, vol. 24, pp. 1750--1762, 2021.

\bibitem{Goodfellow16}
I. Goodfellow, Y. Bengio, and A. Courville,
\textit{Deep Learning}.
MIT Press, 2016.

\bibitem{LeCun15}
Y. LeCun, Y. Bengio, and G. Hinton,
``Deep learning,''
\textit{Nature}, vol. 521, no. 7553, pp. 436--444, 2015.

\bibitem{Krizhevsky12}
A. Krizhevsky, I. Sutskever, and G. E. Hinton,
``ImageNet classification with deep convolutional neural networks,''
in \textit{Advances in Neural Information Processing Systems}, 2012, pp. 1097--1105.

\bibitem{Hochreiter97}
S. Hochreiter and J. Schmidhuber,
``Long short-term memory,''
\textit{Neural Computation}, vol. 9, no. 8, pp. 1735--1780, 1997.

\bibitem{Camgoz20}
N. C. Camgoz, S. Hadfield, O. Koller, H. Ney, and R. Bowden,
``Neural sign language translation,''
in \textit{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 2018, pp. 7784--7793.

\bibitem{Koller20}
O. Koller, S. Zargaran, H. Ney, and R. Bowden,
``Deep sign: Enabling robust statistical continuous sign language recognition via hybrid CNN-HMMs,''
\textit{International Journal of Computer Vision}, vol. 126, no. 12, pp. 1311--1325, 2018.

\bibitem{Ong05}
S. C. W. Ong and S. Ranganath,
``Automatic sign language analysis: A survey and the future beyond lexical meaning,''
\textit{IEEE Transactions on Pattern Analysis and Machine Intelligence}, vol. 27, no. 6, pp. 873--891, 2005.

\bibitem{Huang18}
J. Huang, W. Zhou, H. Li, and W. Li,
``Attention-based 3D-CNNs for large-vocabulary sign language recognition,''
\textit{IEEE Transactions on Circuits and Systems for Video Technology}, vol. 29, no. 9, pp. 2822--2832, 2018.

\bibitem{Tran15}
D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri,
``Learning spatiotemporal features with 3D convolutional networks,''
in \textit{Proceedings of the IEEE International Conference on Computer Vision}, 2015, pp. 4489--4497.

\bibitem{Simonyan14}
K. Simonyan and A. Zisserman,
``Two-stream convolutional networks for action recognition in videos,''
in \textit{Advances in Neural Information Processing Systems}, 2014, pp. 568--576.

\bibitem{Bragg19}
D. Bragg, O. Koller, M. Bellard, L. Berke, P. Boudreault, A. Braffort, N. Camgoz, S. Dilsizian, J. Ecoffet, K. Kacorri, D. Verhoeven, F. Weise, and M. Weinstock,
``Sign language recognition, generation, and translation: An interdisciplinary perspective,''
in \textit{Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility}, 2019, pp. 16--31.

\bibitem{Zhang20}
F. Zhang, V. Bazarevsky, A. Vakunov, A. Tkachenka, G. Sung, C. Chang, and M. Grundmann,
``MediaPipe hands: On-device real-time hand tracking,''
\textit{arXiv preprint arXiv:2006.10214}, 2020.

\bibitem{Ultralytics23}
Ultralytics,
``YOLOv8: A new state-of-the-art computer vision model,''
Available: \url{https://github.com/ultralytics/ultralytics}, 2023.

\end{thebibliography}

\end{document}
