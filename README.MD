# ğŸ–ï¸ Reconnaissance de Langue des Signes - Classification d'Actions

## ğŸ“‹ Table des MatiÃ¨res

1. [Vue d'ensemble du Projet](#vue-densemble-du-projet)
2. [Architecture Technique](#architecture-technique)
3. [MÃ©thodologie et Approche](#mÃ©thodologie-et-approche)
4. [PrÃ©processing des DonnÃ©es](#prÃ©processing-des-donnÃ©es)
5. [Architecture du ModÃ¨le](#architecture-du-modÃ¨le)
6. [RÃ©sultats et Performance](#rÃ©sultats-et-performance)
7. [Application Web Flask](#application-web-flask)
8. [Installation et Utilisation](#installation-et-utilisation)
9. [Structure du Projet](#structure-du-projet)
10. [Technologies UtilisÃ©es](#technologies-utilisÃ©es)

---

## ğŸ¯ Vue d'ensemble du Projet

Ce projet implÃ©mente un systÃ¨me de reconnaissance de langue des signes basÃ© sur l'intelligence artificielle, capable de classifier des actions/signes Ã  partir de vidÃ©os. Le systÃ¨me utilise une combinaison de dÃ©tection d'objets (YOLOv8), d'extraction de points clÃ©s (MediaPipe), et d'apprentissage profond (LSTM) pour identifier quatre classes d'objets informatiques : **clavier**, **disque dur**, **ordinateur**, et **souris**.

### ğŸ¯ Contexte et Motivation
La langue des signes est un langage visuel qui sert de moyen de communication principal pour des millions de personnes sourdes et malentendantes dans le monde. Elle utilise des gestes des mains, des expressions faciales et des mouvements corporels pour transmettre du sens, formant des systÃ¨mes linguistiques complexes aussi riches et expressifs que les langues parlÃ©es. La reconnaissance et l'interprÃ©tation de la langue des signes par l'intelligence artificielle reprÃ©sente une avancÃ©e critique dans les technologies d'accessibilitÃ©, avec le potentiel de combler les Ã©carts de communication et de crÃ©er des environnements numÃ©riques plus inclusifs.

### ğŸ¯ Objectifs Principaux
- **DÃ©velopper un Pipeline Complet** : Concevoir et implÃ©menter un systÃ¨me complet qui traite les vidÃ©os brutes via la dÃ©tection des mains, l'extraction de points clÃ©s, la modÃ©lisation de sÃ©quences et la classification pour reconnaÃ®tre les gestes de langue des signes
- **ImplÃ©menter des Techniques de Vision par Ordinateur de Pointe** : Exploiter YOLOv8 pour une dÃ©tection robuste des mains et MediaPipe pour une extraction prÃ©cise des points clÃ©s
- **Concevoir un ModÃ¨le de Classification de SÃ©quences Efficace** : DÃ©velopper et entraÃ®ner un rÃ©seau neuronal basÃ© sur LSTM capable d'apprendre les motifs temporels dans les sÃ©quences de points clÃ©s
- **CrÃ©er une Solution de DÃ©ploiement Pratique** : ImplÃ©menter Ã  la fois un pipeline d'entraÃ®nement complet et une application web conviviale
- **Ã‰valuer et Valider les Performances** : Effectuer une Ã©valuation complÃ¨te en utilisant des mÃ©triques standard d'apprentissage automatique
- **Assurer l'AccessibilitÃ© et la ConfidentialitÃ©** : DÃ©velopper un systÃ¨me capable de fonctionner hors ligne qui respecte la confidentialitÃ© des utilisateurs

---

## ğŸ—ï¸ Architecture Technique

### Pipeline de Traitement
```
VidÃ©o d'entrÃ©e â†’ DÃ©tection YOLO â†’ Extraction MediaPipe â†’ SÃ©quence LSTM â†’ PrÃ©diction
```

### Composants Principaux
1. **YOLOv8** : DÃ©tection des rÃ©gions d'intÃ©rÃªt (mains/personnes)
2. **MediaPipe Hands** : Extraction de 21 points clÃ©s par main
3. **LSTM** : ModÃ¨le de sÃ©quence pour la classification temporelle
4. **Flask** : Interface web pour l'interaction utilisateur

---

## ğŸ”¬ MÃ©thodologie et Approche

### 1. DÃ©tection et Localisation
- **YOLOv8n** : ModÃ¨le lÃ©ger pour la dÃ©tection de personnes/mains
- **Seuil de confiance** : 0.4 pour filtrer les dÃ©tections faibles
- **ROI (Region of Interest)** : Extraction des zones contenant les mains

### 2. Extraction de Points ClÃ©s
- **MediaPipe Hands** : 21 points clÃ©s par main (x, y normalisÃ©s)
- **Support multi-mains** : Jusqu'Ã  2 mains simultanÃ©ment
- **CoordonnÃ©es normalisÃ©es** : Adaptation Ã  diffÃ©rentes rÃ©solutions

### 3. Traitement Temporel
- **SÃ©quence fixe** : 30 frames par vidÃ©o
- **Padding/Truncation** : Standardisation de la longueur
- **Features par frame** : 84 dimensions (2 mains Ã— 21 points Ã— 2 coordonnÃ©es)

### 4. DÃ©fis Techniques AdressÃ©s
- **ComplexitÃ© Temporelle** : Les gestes de langue des signes sont intrinsÃ¨quement temporels, nÃ©cessitant l'analyse de motifs de mouvement dans le temps
- **VariabilitÃ© Spatiale** : Les positions, orientations et Ã©chelles des mains peuvent varier considÃ©rablement entre individus et conditions d'enregistrement
- **Datasets LimitÃ©s** : Contrairement aux autres tÃ¢ches de vision par ordinateur, les datasets de langue des signes Ã  grande Ã©chelle sont relativement rares
- **Exigences de Traitement Temps RÃ©el** : Les applications pratiques nÃ©cessitent des capacitÃ©s d'infÃ©rence en temps rÃ©el tout en maintenant une prÃ©cision Ã©levÃ©e
- **AccessibilitÃ© et DÃ©ploiement** : Les systÃ¨mes doivent Ãªtre dÃ©ployables dans divers environnements avec des ressources informatiques variables

---

## ğŸ“Š PrÃ©processing des DonnÃ©es

### Configuration des ParamÃ¨tres
```python
# ParamÃ¨tres MediaPipe
MAX_NUM_HANDS_MEDIAPIPE = 2
MIN_DETECTION_CONF_MEDIAPIPE = 0.5
MIN_TRACKING_CONF_MEDIAPIPE = 0.5

# ParamÃ¨tres de sÃ©quence
NUM_FRAMES_PER_VIDEO = 30
NUM_KEYPOINTS_PER_HAND = 21
NUM_COORDS_PER_KEYPOINT = 2
INPUT_SIZE = 84  # 2 Ã— 21 Ã— 2
```

### Pipeline de PrÃ©processing
1. **Extraction vidÃ©o** : Lecture frame par frame
2. **DÃ©tection YOLO** : Identification des ROIs
3. **Extraction MediaPipe** : Points clÃ©s des mains
4. **Normalisation** : CoordonnÃ©es relatives Ã  l'image
5. **SÃ©quenÃ§age** : Padding/truncation Ã  30 frames
6. **Stockage HDF5** : Format optimisÃ© pour l'apprentissage

### Dataset Final
- **199 vidÃ©os** au total
- **4 classes** : clavier, disque dur, ordinateur, souris
- **RÃ©partition** : 139 train / 40 validation / 20 test
- **Format** : HDF5 avec mÃ©tadonnÃ©es JSON

### CaractÃ©ristiques des VidÃ©os
- **DurÃ©e** : Variable (typiquement 2-5 secondes)
- **RÃ©solution** : Variable (standardisÃ©e Ã  640Ã—640 pendant le prÃ©traitement)
- **Taux de frames** : Variable (traitÃ© Ã  intervalles fixes)
- **Format** : MP4 avec encodage H.264
- **Contenu** : Individu unique effectuant des gestes de signes isolÃ©s

---

## ğŸ§  Architecture du ModÃ¨le

### LSTMClassifier
```python
class LSTMClassifier(L.LightningModule):
    def __init__(self, input_size=84, hidden_size=256, num_layers=2, 
                 num_classes=4, dropout=0.3, learning_rate=1e-3, weight_decay=1e-4):
```

### CaractÃ©ristiques du ModÃ¨le
- **Type** : LSTM bidirectionnel
- **Couches cachÃ©es** : 256 unitÃ©s
- **Nombre de couches** : 2
- **Dropout** : 0.3 pour la rÃ©gularisation
- **ParamÃ¨tres** : 877K paramÃ¨tres entraÃ®nables
- **Taille estimÃ©e** : 3.51 MB

### Optimisation
- **Optimiseur** : AdamW avec weight decay
- **Scheduler** : ReduceLROnPlateau
- **Monitoring** : F1-score macro pour la validation
- **Early stopping** : Patience de 10 Ã©poques

---

## ğŸ“ˆ RÃ©sultats et Performance

### MÃ©triques Finales (Test Set)
```
Test Accuracy: 80.0%
Test F1-Score (Macro): 78.7%
Test Loss: 0.814
```

### Performance d'InfÃ©rence
```
Temps de Traitement par VidÃ©o : â‰ˆ 1.8 secondes
Extraction de Frames VidÃ©o : 0.12s par vidÃ©o
DÃ©tection des Mains (YOLOv8) : 0.03s par frame
Extraction de Points ClÃ©s (MediaPipe) : 0.02s par frame
Classification de SÃ©quence (LSTM) : 0.01s par sÃ©quence
```

### Rapport de Classification DÃ©taillÃ©
```
              precision    recall  f1-score   support

     clavier       1.00      1.00      1.00         5
  disque_dur       0.57      0.80      0.67         5
  ordinateur       0.83      1.00      0.91         5
      souris       1.00      0.40      0.57         5

    accuracy                           0.80        20
   macro avg       0.85      0.80      0.79        20
weighted avg       0.85      0.80      0.79        20
```

### Analyse des Performances
- **Clavier** : Performance parfaite (100% precision/recall)
- **Ordinateur** : Excellente performance (91% F1-score)
- **Disque dur** : Performance moyenne (67% F1-score)
- **Souris** : Performance limitÃ©e (57% F1-score)

### ModÃ¨les SauvegardÃ©s
- **Meilleur modÃ¨le** : `best-action-model-epoch=31-val_f1=0.92.ckpt`
- **F1-score validation** : 92%
- **Ã‰poques d'entraÃ®nement** : 31 (early stopping)

### Comparaison avec l'Ã‰tat de l'Art
Les rÃ©sultats de 80% de prÃ©cision s'alignent favorablement avec les prÃ©cisions rapportÃ©es dans la littÃ©rature. Les Ã©tudes utilisant des approches similaires basÃ©es sur les points clÃ©s ont rapportÃ© des prÃ©cisions allant de 75-90% sur des tÃ¢ches de vocabulaire limitÃ©. La prÃ©cision de 80% atteinte dans ce travail se situe dans cette fourchette tout en utilisant une architecture de modÃ¨le relativement compacte et efficace.

---

## ğŸŒ Application Web Flask

### Architecture de l'Application
```python
# Pipeline de prÃ©diction dans app.py
def predict_sign(video_path):
    1. process_video_for_inference()  # Extraction des points clÃ©s
    2. Conversion en tensor          # PrÃ©paration pour le modÃ¨le
    3. Inference LSTM               # PrÃ©diction
    4. Softmax                     # ProbabilitÃ©s
    5. Retour des rÃ©sultats        # Classe + confiance
```

### FonctionnalitÃ©s
- **Upload vidÃ©o** : Support MP4, AVI, MOV, MKV, WEBM
- **Taille maximale** : 16 MB
- **PrÃ©diction temps rÃ©el** : Traitement immÃ©diat
- **Nettoyage automatique** : Suppression des fichiers temporaires
- **Gestion d'erreurs** : Messages d'erreur dÃ©taillÃ©s
- **Interface responsive** : Compatible desktop et mobile
- **Traitement hors ligne** : Aucune dÃ©pendance cloud requise
- **Scores de confiance** : Affichage des probabilitÃ©s pour chaque classe

### Interface Utilisateur
- **Template HTML** : Interface moderne et responsive
- **Feedback visuel** : Affichage des rÃ©sultats
- **Gestion des erreurs** : Messages utilisateur clairs

---

## ğŸš€ Installation et Utilisation

### PrÃ©requis
```bash
# Python 3.9+
# CUDA (optionnel, pour GPU)
# 8GB+ RAM recommandÃ©
```

### Installation
```bash
# 1. Cloner le repository
git clone <repository-url>
cd sign-lang-classification

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. VÃ©rifier les fichiers requis
python run_app.py
```

### Fichiers Requis
- `yolov8n.pt` : ModÃ¨le YOLO prÃ©-entraÃ®nÃ©
- `data-yolo/model.h5` : Dataset prÃ©processÃ©
- `model_yolo/best-action-model-*.ckpt` : ModÃ¨le entraÃ®nÃ©

### Lancement de l'Application
```bash
# MÃ©thode 1 : Script de lancement
python run_app.py

# MÃ©thode 2 : Direct
python app.py

# AccÃ¨s : http://localhost:5000
```

---

## ğŸ“ Structure du Projet

```
sign-lang-classification/
â”œâ”€â”€ ğŸ“„ app.py                    # Application Flask principale
â”œâ”€â”€ ğŸ“„ run_app.py               # Script de lancement avec vÃ©rifications
â”œâ”€â”€ ğŸ“„ requirements.txt         # DÃ©pendances Python
â”œâ”€â”€ ğŸ“„ README.MD               # Documentation (ce fichier)
â”œâ”€â”€ ğŸ“„ yolov8n.pt              # ModÃ¨le YOLO prÃ©-entraÃ®nÃ©
â”‚
â”œâ”€â”€ ğŸ“ SignLanguageDataset/     # Dataset vidÃ©o brut
â”‚   â”œâ”€â”€ ğŸ“ clavier/            # 50 vidÃ©os
â”‚   â”œâ”€â”€ ğŸ“ disque_dur/         # 50 vidÃ©os
â”‚   â”œâ”€â”€ ğŸ“ ordinateur/         # 50 vidÃ©os
â”‚   â””â”€â”€ ğŸ“ souris/             # 49 vidÃ©os
â”‚
â”œâ”€â”€ ğŸ“ data-yolo/              # DonnÃ©es prÃ©processÃ©es
â”‚   â””â”€â”€ ğŸ“„ model.h5            # Dataset HDF5
â”‚
â”œâ”€â”€ ğŸ“ model_yolo/             # ModÃ¨les entraÃ®nÃ©s
â”‚   â”œâ”€â”€ ğŸ“„ best-action-model-epoch=31-val_f1=0.92.ckpt
â”‚   â”œâ”€â”€ ğŸ“„ best-action-model-epoch=13-val_f1=0.88-v3.ckpt
â”‚   â””â”€â”€ ğŸ“ action_rec_logs/    # Logs d'entraÃ®nement
â”‚
â”œâ”€â”€ ğŸ“ templates/              # Templates Flask
â”‚   â””â”€â”€ ğŸ“„ index.html          # Interface utilisateur
â”‚
â”œâ”€â”€ ğŸ“ uploads/                # Fichiers temporaires
â””â”€â”€ ğŸ“„ model - FINAL .ipynb    # Notebook d'entraÃ®nement
```

---

## ğŸ› ï¸ Technologies UtilisÃ©es

### Intelligence Artificielle
- **PyTorch** : Framework de deep learning
- **PyTorch Lightning** : Organisation du code d'entraÃ®nement
- **Ultralytics YOLO** : DÃ©tection d'objets
- **MediaPipe** : Extraction de points clÃ©s
- **TorchMetrics** : MÃ©triques d'Ã©valuation

### Traitement de DonnÃ©es
- **OpenCV** : Traitement vidÃ©o et image
- **NumPy** : Calculs numÃ©riques
- **H5Py** : Stockage de donnÃ©es optimisÃ©
- **Pandas** : Manipulation de donnÃ©es

### Web et Interface
- **Flask** : Framework web
- **Werkzeug** : Utilitaires web
- **HTML/CSS/JavaScript** : Interface utilisateur

### Utilitaires
- **Scikit-learn** : MÃ©triques et validation
- **Matplotlib** : Visualisation
- **TQDM** : Barres de progression

---

## ğŸ” DÃ©tails Techniques AvancÃ©s

### Optimisations ImplÃ©mentÃ©es
1. **DÃ©tection ROI** : YOLO pour rÃ©duire la zone de traitement
2. **Multi-mains** : Support simultanÃ© de 2 mains
3. **Normalisation** : CoordonnÃ©es relatives pour la robustesse
4. **Padding intelligent** : Gestion des vidÃ©os de longueurs variables
5. **Early stopping** : PrÃ©vention du surapprentissage
6. **Learning rate scheduling** : Adaptation automatique du taux d'apprentissage

### Gestion de la MÃ©moire
- **Batch processing** : Traitement par lots pour les vidÃ©os longues
- **Nettoyage automatique** : Suppression des fichiers temporaires
- **Optimisation GPU** : Utilisation efficace de la mÃ©moire CUDA

### Robustesse
- **Gestion d'erreurs** : Traitement des cas d'Ã©chec
- **Validation des entrÃ©es** : VÃ©rification des formats de fichiers
- **Fallback** : DÃ©tection sur image complÃ¨te si YOLO Ã©choue

---

## ğŸ“ Notes de DÃ©veloppement

### DÃ©fis RencontrÃ©s
1. **Synchronisation multi-mains** : Gestion des mains gauche/droite
2. **VariabilitÃ© temporelle** : Standardisation des sÃ©quences
3. **Performance temps rÃ©el** : Optimisation pour l'interface web
4. **Gestion mÃ©moire** : Traitement de vidÃ©os volumineuses
5. **Taille limitÃ©e du dataset** : 199 vidÃ©os pour 4 classes
6. **VariabilitÃ© des conditions d'enregistrement** : Ã‰clairage, angles de camÃ©ra, styles de signes

### Limitations Actuelles
- **ReprÃ©sentation des caractÃ©ristiques** : Utilisation uniquement des points clÃ©s des mains, ignorant les expressions faciales et la posture corporelle
- **ModÃ©lisation temporelle** : Longueur de sÃ©quence fixe de 30 frames peut ne pas capturer optimalement la structure temporelle naturelle des signes
- **Architecture du modÃ¨le** : Les rÃ©seaux LSTM ont des limitations inhÃ©rentes pour les dÃ©pendances Ã  long terme
- **SensibilitÃ© aux conditions** : Performance dÃ©gradÃ©e avec des angles de camÃ©ra inhabituels, un Ã©clairage mÃ©diocre ou des occlusions

### AmÃ©liorations Futures
1. **Expansion du Dataset** : EntraÃ®ner le modÃ¨le sur un dataset beaucoup plus large et diversifiÃ©
2. **IntÃ©gration Multi-Modale** : Incorporer les points clÃ©s du visage et de la posture corporelle
3. **Architectures AvancÃ©es** : Explorer les Transformers, GRUs ou les rÃ©seaux de convolution temporelle
4. **Augmentation de DonnÃ©es** : Techniques d'augmentation au niveau des points clÃ©s
5. **InfÃ©rence Temps RÃ©el** : Optimisation du pipeline pour l'infÃ©rence webcam en temps rÃ©el
6. **Reconnaissance Continue** : Extension de la reconnaissance de signes isolÃ©s Ã  la langue des signes continue
7. **Personnalisation** : SystÃ¨mes adaptatifs qui apprennent les styles de signes individuels

---

## ğŸ“ Support et Contact

Pour toute question ou problÃ¨me :
- **Issues GitHub** : Signaler les bugs
- **Documentation** : Consulter ce README
- **Exemples** : Voir le notebook d'entraÃ®nement

## ğŸ“ Informations AcadÃ©miques

### Auteurs
- **Othmane FERRAH**
- **Hicham BENLMAHI** 
- **Soufian EL KARCHAL**

### Institution
- **DÃ©partement** : Informatique et GÃ©nie Informatique
- **Programme** : Licence en Sciences MathÃ©matiques et Informatique (SMI)
- **AnnÃ©e AcadÃ©mique** : 2024/2025

### Projet
- **Type** : Projet de fin d'Ã©tudes
- **Sujet** : DÃ©veloppement d'un SystÃ¨me de Reconnaissance de Langue des Signes en Temps RÃ©el Utilisant l'Apprentissage Profond et la Vision par Ordinateur
- **Supervision** : Pr. [Nom du Superviseur]

## ğŸ“š RÃ©fÃ©rences Principales

Ce projet s'appuie sur les travaux de recherche suivants :
- **Adaloglou et al. (2021)** : Ã‰tude complÃ¨te sur les mÃ©thodes d'apprentissage profond pour la reconnaissance de langue des signes
- **Camgoz et al. (2020)** : Traduction neuronale de langue des signes
- **Zhang et al. (2020)** : MediaPipe Hands pour le suivi des mains en temps rÃ©el
- **Ultralytics (2023)** : YOLOv8, nouveau modÃ¨le de vision par ordinateur de pointe

---

*Projet dÃ©veloppÃ© avec â¤ï¸ pour la reconnaissance de langue des signes*
