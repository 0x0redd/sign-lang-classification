{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69be578",
   "metadata": {},
   "source": [
    "## MSKA: Multi-Stream Keypoint-based Action Recognition for Sign Language (Hands Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead10607",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm \n",
    "from torchmetrics import F1Score, Accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from einops import rearrange \n",
    "\n",
    "DATA_DIR = \"SignLanguage_Processed_Data/keypoints_for_model\"  # <--- UPDATE THIS PATH\n",
    "NUM_CLASSES = 4  # <--- UPDATE THIS for \"clavier, disque dur, ordinateur, souris\"\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50 \n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "D_MODEL = 128 \n",
    "N_HEAD = 4    \n",
    "NUM_LAYERS = 2 \n",
    "\n",
    "LEFT_HAND_INDICES = slice(501, 522)\n",
    "RIGHT_HAND_INDICES = slice(522, 543)\n",
    "NUM_HAND_KEYPOINTS = 21\n",
    "KEYPOINT_FEATURES = 3 \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135c652",
   "metadata": {},
   "source": [
    "### 2. Dataset Class and Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c8c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageKeypointsDataset(Dataset):\n",
    "    def __init__(self, data_samples, labels, max_frames):\n",
    "        self.data_samples = data_samples\n",
    "        self.labels = labels\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.data_samples[idx]\n",
    "        label = self.labels[idx]\n",
    "        try:\n",
    "            holistic_data = np.load(file_path)\n",
    "            # Basic sanity check for loaded data shape\n",
    "            if not (holistic_data.ndim == 3 and holistic_data.shape[1] == 543 and holistic_data.shape[2] == KEYPOINT_FEATURES and holistic_data.shape[0] > 0):\n",
    "                print(f\"Warning: File {file_path} has unexpected shape {holistic_data.shape} or zero frames. Returning zeros.\")\n",
    "                holistic_data = np.zeros((self.max_frames, 543, KEYPOINT_FEATURES), dtype=np.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}. Returning zeros.\")\n",
    "            holistic_data = np.zeros((self.max_frames, 543, KEYPOINT_FEATURES), dtype=np.float32)\n",
    "\n",
    "        left_hand_kps = holistic_data[:, LEFT_HAND_INDICES, :]\n",
    "        right_hand_kps = holistic_data[:, RIGHT_HAND_INDICES, :]\n",
    "\n",
    "        left_hand_kps = self.pad_or_truncate_stream(left_hand_kps, NUM_HAND_KEYPOINTS)\n",
    "        right_hand_kps = self.pad_or_truncate_stream(right_hand_kps, NUM_HAND_KEYPOINTS)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(left_hand_kps, dtype=torch.float32),\n",
    "            torch.tensor(right_hand_kps, dtype=torch.float32),\n",
    "            torch.tensor(label, dtype=torch.long),\n",
    "        )\n",
    "\n",
    "    def pad_or_truncate_stream(self, stream, num_keypoints_per_stream):\n",
    "        current_frames = stream.shape[0]\n",
    "        if current_frames == 0: \n",
    "            return np.zeros((self.max_frames, num_keypoints_per_stream, KEYPOINT_FEATURES), dtype=np.float32)\n",
    "        if current_frames == self.max_frames:\n",
    "            return stream\n",
    "        elif current_frames < self.max_frames:\n",
    "            padding_shape = (self.max_frames - current_frames, num_keypoints_per_stream, KEYPOINT_FEATURES)\n",
    "            padding = np.zeros(padding_shape, dtype=stream.dtype)\n",
    "            return np.concatenate([stream, padding], axis=0)\n",
    "        else:\n",
    "            return stream[:self.max_frames, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42466cad",
   "metadata": {},
   "source": [
    "### 3. Model Architecture (Hands Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        batch, frames, num_kps, d_model_feat = x.shape\n",
    "        x_reshaped = x.reshape(batch * frames, num_kps, d_model_feat)\n",
    "        Q, K, V = self.query(x_reshaped), self.key(x_reshaped), self.value(x_reshaped)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_model_feat**0.5)\n",
    "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
    "        out = torch.matmul(attn_probs, V)\n",
    "        return out.reshape(batch, frames, num_kps, d_model_feat)\n",
    "\n",
    "class TemporalTransformer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers, dim_feedforward_factor=4):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, \n",
    "                                               dim_feedforward=d_model * dim_feedforward_factor, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.transformer_encoder(x)\n",
    "        return self.norm(x)\n",
    "\n",
    "class ConvTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_transformer_layers):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model * 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(d_model * 2, d_model, kernel_size=3, padding=1))\n",
    "        self.norm_after_conv = nn.LayerNorm(d_model)\n",
    "        self.transformer = TemporalTransformer(d_model, nhead, num_transformer_layers)\n",
    "    def forward(self, x):\n",
    "        x_permuted = x.permute(0, 2, 1)\n",
    "        conv_out = self.conv_block(x_permuted)\n",
    "        conv_out_permuted = conv_out.permute(0, 2, 1)\n",
    "        x_residual = x + conv_out_permuted \n",
    "        x_normed = self.norm_after_conv(x_residual) \n",
    "        return self.transformer(x_normed)\n",
    "\n",
    "class HandStreamProcessor(nn.Module):\n",
    "    def __init__(self, in_keypoint_features, num_keypoints, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(nn.Linear(in_keypoint_features, d_model), nn.ReLU(), nn.LayerNorm(d_model))\n",
    "        self.spatial_attn = SpatialAttention(d_model)\n",
    "        self.temporal_conv_transformer = ConvTransformerBlock(d_model, nhead, num_layers)\n",
    "        self.temporal_attn_pool = nn.Linear(d_model, 1) \n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.spatial_attn(x)\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.temporal_conv_transformer(x)\n",
    "        attn_weights = F.softmax(self.temporal_attn_pool(x).squeeze(-1), dim=-1)\n",
    "        return torch.sum(x * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "class SignLanguageHandsModel(nn.Module):\n",
    "    def __init__(self, num_classes, in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers):\n",
    "        super().__init__()\n",
    "        self.left_hand_processor = HandStreamProcessor(in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers)\n",
    "        self.right_hand_processor = HandStreamProcessor(in_keypoint_features, num_hand_keypoints, d_model, nhead, num_layers)\n",
    "        self.classifier = nn.Sequential(nn.Linear(d_model * 2, d_model), nn.ReLU(), nn.Dropout(0.3), nn.Linear(d_model, num_classes))\n",
    "    def forward(self, left_hand_input, right_hand_input):\n",
    "        left_out = self.left_hand_processor(left_hand_input)\n",
    "        right_out = self.right_hand_processor(right_hand_input)\n",
    "        combined_features = torch.cat([left_out, right_out], dim=-1)\n",
    "        return self.classifier(combined_features)\n",
    "\n",
    "IN_KEYPOINT_FEATURES = KEYPOINT_FEATURES\n",
    "NUM_KP_PER_HAND = NUM_HAND_KEYPOINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2f913",
   "metadata": {},
   "source": [
    "### 4. Loss Function (Balanced Focal Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13980f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_counts, gamma=2.0, beta=0.9999):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        if not isinstance(class_counts, torch.Tensor):\n",
    "            class_counts = torch.tensor(class_counts, dtype=torch.float)\n",
    "        effective_num = 1.0 - torch.pow(beta, class_counts)\n",
    "        weights = (1.0 - beta) / torch.clamp(effective_num, min=1e-8) \n",
    "        weights = weights / weights.sum() * len(class_counts) \n",
    "        self.register_buffer('weights', weights)\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_term = (1 - pt) ** self.gamma\n",
    "        alpha = self.weights[targets].to(inputs.device) \n",
    "        balanced_focal_loss = alpha * focal_term * ce_loss\n",
    "        return balanced_focal_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03771e",
   "metadata": {},
   "source": [
    "### 5. Training and Validation Loop Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb66d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, current_train_loader, current_val_loader, num_classes, device, num_epochs, learning_rate, weight_decay, current_y_train_labels):\n",
    "    train_f1_metric = F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    val_f1_metric = F1Score(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "    train_accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "    val_accuracy_metric = Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=7, factor=0.5, verbose=True) # Increased patience\n",
    "\n",
    "    if len(current_y_train_labels) == 0:\n",
    "        raise ValueError(\"current_y_train_labels is empty.\")\n",
    "    class_counts_np = np.bincount(current_y_train_labels, minlength=num_classes)\n",
    "    class_counts_tensor = torch.tensor(class_counts_np, dtype=torch.float).to(device)\n",
    "    criterion = BalancedFocalLoss(class_counts=class_counts_tensor, gamma=2.0)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_f1': [], 'val_f1': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0.0\n",
    "        train_pbar = tqdm(current_train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=False)\n",
    "        \n",
    "        for left_kps, right_kps, labels in train_pbar:\n",
    "            left_kps, right_kps, labels = left_kps.to(device), right_kps.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(left_kps, right_kps)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            train_f1_metric.update(outputs, labels)\n",
    "            train_accuracy_metric.update(outputs, labels)\n",
    "            train_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(current_train_loader)\n",
    "        epoch_train_f1 = train_f1_metric.compute().item()\n",
    "        epoch_train_acc = train_accuracy_metric.compute().item()\n",
    "        train_f1_metric.reset(); train_accuracy_metric.reset()\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "        val_pbar = tqdm(current_val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]', leave=False)\n",
    "        with torch.no_grad():\n",
    "            for left_kps, right_kps, labels in val_pbar:\n",
    "                left_kps, right_kps, labels = left_kps.to(device), right_kps.to(device), labels.to(device)\n",
    "                outputs = model(left_kps, right_kps)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_val_loss += loss.item()\n",
    "                val_f1_metric.update(outputs, labels)\n",
    "                val_accuracy_metric.update(outputs, labels)\n",
    "                val_pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(current_val_loader)\n",
    "        epoch_val_f1 = val_f1_metric.compute().item()\n",
    "        epoch_val_acc = val_accuracy_metric.compute().item()\n",
    "        val_f1_metric.reset(); val_accuracy_metric.reset()\n",
    "\n",
    "        history['train_loss'].append(avg_train_loss); history['val_loss'].append(avg_val_loss)\n",
    "        history['train_f1'].append(epoch_train_f1); history['val_f1'].append(epoch_val_f1)\n",
    "        history['train_acc'].append(epoch_train_acc); history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}, Train F1: {epoch_train_f1:.4f}, Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"  Val Loss:   {avg_val_loss:.4f}, Val F1:   {epoch_val_f1:.4f}, Val Acc:   {epoch_val_acc:.4f}\")\n",
    "        scheduler.step(epoch_val_f1)\n",
    "        if epoch_val_f1 > best_val_f1:\n",
    "            best_val_f1 = epoch_val_f1\n",
    "            torch.save(model.state_dict(), 'best_sign_model_hands_only.pth')\n",
    "            print(f\"  🔥 New best model saved with Val F1: {best_val_f1:.4f}\")\n",
    "\n",
    "    print(\"--- Training Finished ---\")\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 3, 1); plt.plot(history['train_loss'], label='Train Loss'); plt.plot(history['val_loss'], label='Val Loss'); plt.title('Loss Evolution'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 3, 2); plt.plot(history['train_f1'], label='Train F1'); plt.plot(history['val_f1'], label='Val F1'); plt.title('F1 Score Evolution'); plt.xlabel('Epoch'); plt.ylabel('F1 Score'); plt.legend(); plt.grid(True)\n",
    "    plt.subplot(1, 3, 3); plt.plot(history['train_acc'], label='Train Accuracy'); plt.plot(history['val_acc'], label='Val Accuracy'); plt.title('Accuracy Evolution'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True)\n",
    "    plt.tight_layout(); plt.savefig('sign_language_training_hands_only.png'); plt.show()\n",
    "\n",
    "    model.load_state_dict(torch.load('best_sign_model_hands_only.pth'))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff0f88",
   "metadata": {},
   "source": [
    "### 6. Evaluation Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "def evaluate_model(model, current_test_loader, device, current_label_mapping, num_classes_eval):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    test_pbar = tqdm(current_test_loader, desc='Evaluating on Test Set', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for left_kps, right_kps, labels in test_pbar:\n",
    "            left_kps, right_kps = left_kps.to(device), right_kps.to(device)\n",
    "            outputs = model(left_kps, right_kps)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    unique_labels_in_test = sorted(list(set(all_labels)))\n",
    "    target_names_report = [name for name, idx in sorted(current_label_mapping.items(), key=lambda item: item[1]) if idx in unique_labels_in_test]\n",
    "    cm_target_names = [name for name, idx in sorted(current_label_mapping.items(), key=lambda item: item[1])]\n",
    "    cm_labels = list(range(num_classes_eval))\n",
    "\n",
    "    print(\"\\n--- Test Set Evaluation ---\")\n",
    "    print(\"\\n📊 Classification Report:\")\n",
    "    if not target_names_report and unique_labels_in_test:\n",
    "        print(\"Warning: target_names_report is empty, using stringified labels for report.\")\n",
    "        target_names_report = [str(l) for l in unique_labels_in_test]\n",
    "    \n",
    "    if not unique_labels_in_test:\n",
    "        print(\"No labels found in the test set for evaluation.\")\n",
    "        return {'accuracy': 0, 'f1_macro': 0, 'f1_weighted': 0, 'classification_report': \"No labels\", 'confusion_matrix': np.array([])}\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, labels=unique_labels_in_test, target_names=target_names_report, digits=4, zero_division=0))\n",
    "    \n",
    "    test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "    test_f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    test_f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\n🔍 Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"🔍 Test F1-Score (Macro): {test_f1_macro:.4f}\")\n",
    "    print(f\"🔍 Test F1-Score (Weighted): {test_f1_weighted:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=cm_labels)\n",
    "    plt.figure(figsize=(max(10, num_classes_eval // 1.5), max(8, num_classes_eval // 2)))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cm_target_names, yticklabels=cm_target_names)\n",
    "    plt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n",
    "    plt.tight_layout(); plt.savefig('confusion_matrix_hands_only.png'); plt.show()\n",
    "    \n",
    "    return {'accuracy': test_accuracy, 'f1_macro': test_f1_macro, 'f1_weighted': test_f1_weighted}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2cf41",
   "metadata": {},
   "source": [
    "### 7. Main Execution Block (Data Loading, Training, Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b22af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell groups the main logic for notebook execution.\n",
    "# It relies on variables defined in the first cell (DATA_DIR, NUM_CLASSES, etc.)\n",
    "# and class/function definitions from the cells above.\n",
    "\n",
    "# --- Load data paths, labels, and create DataLoaders (copied from cell 2 for self-containment) ---\n",
    "all_file_paths = []\n",
    "all_labels = []\n",
    "label_mapping = {} # Will be populated here\n",
    "current_label_idx = 0\n",
    "frame_counts = []\n",
    "\n",
    "print(f\"Loading data from: {DATA_DIR}\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}. Please run preprocessing script first.\")\n",
    "    \n",
    "for class_name in sorted(os.listdir(DATA_DIR)):\n",
    "    class_path = os.path.join(DATA_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    if class_name not in label_mapping:\n",
    "        label_mapping[class_name] = current_label_idx\n",
    "        current_label_idx += 1\n",
    "    class_label = label_mapping[class_name]\n",
    "    for split_name in sorted(os.listdir(class_path)):\n",
    "        split_path = os.path.join(class_path, split_name)\n",
    "        if not os.path.isdir(split_path):\n",
    "            continue\n",
    "        for file_name in os.listdir(split_path):\n",
    "            if file_name.endswith(\".npy\"):\n",
    "                file_path = os.path.join(split_path, file_name)\n",
    "                all_file_paths.append(file_path)\n",
    "                all_labels.append(class_label)\n",
    "                try:\n",
    "                    data = np.load(file_path)\n",
    "                    if data.ndim == 3 and data.shape[1] == 543 and data.shape[2] == KEYPOINT_FEATURES and data.shape[0] > 0:\n",
    "                       frame_counts.append(data.shape[0])\n",
    "                    else: \n",
    "                       print(f\"Warning: File {file_path} has unexpected shape {data.shape if hasattr(data, 'shape') else 'N/A'} or zero frames. Marking as problematic.\")\n",
    "                       frame_counts.append(0) # Mark as problematic\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not load {file_path} to get frame count: {e}\")\n",
    "                    frame_counts.append(0)\n",
    "\n",
    "if not all_file_paths:\n",
    "    raise ValueError(f\"No .npy files found in {DATA_DIR}.\")\n",
    "\n",
    "valid_frame_counts = [fc for fc in frame_counts if fc > 0]\n",
    "if valid_frame_counts:\n",
    "    MAX_FRAMES = int(np.percentile(valid_frame_counts, 95))\n",
    "    print(f\"Using MAX_FRAMES = {MAX_FRAMES} (95th percentile of {len(valid_frame_counts)} valid files)\")\n",
    "else:\n",
    "    MAX_FRAMES = 30 \n",
    "    print(f\"Warning: No valid frame counts. Using default MAX_FRAMES = {MAX_FRAMES}\")\n",
    "\n",
    "X_paths = np.array(all_file_paths)\n",
    "y_labels_all = np.array(all_labels)\n",
    "\n",
    "sss_test = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_val_indices, test_indices = next(sss_test.split(X_paths, y_labels_all))\n",
    "X_train_val_paths, X_test_paths = X_paths[train_val_indices], X_paths[test_indices]\n",
    "y_train_val_labels, y_test_labels = y_labels_all[train_val_indices], y_labels_all[test_indices]\n",
    "\n",
    "sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "train_indices, val_indices = next(sss_val.split(X_train_val_paths, y_train_val_labels))\n",
    "X_train_paths, X_val_paths = X_train_val_paths[train_indices], X_train_val_paths[val_indices]\n",
    "y_train_labels, y_val_labels = y_train_val_labels[train_indices], y_train_val_labels[val_indices]\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"Train samples: {len(X_train_paths)}\")\n",
    "print(f\"Validation samples: {len(X_val_paths)}\")\n",
    "print(f\"Test samples: {len(X_test_paths)}\")\n",
    "\n",
    "train_dataset = SignLanguageKeypointsDataset(X_train_paths, y_train_labels, MAX_FRAMES)\n",
    "val_dataset = SignLanguageKeypointsDataset(X_val_paths, y_val_labels, MAX_FRAMES)\n",
    "test_dataset = SignLanguageKeypointsDataset(X_test_paths, y_test_labels, MAX_FRAMES)\n",
    "\n",
    "DEBUG_DATALOADER = False \n",
    "NUM_WORKERS_CFG = 0 if DEBUG_DATALOADER or os.name != 'posix' else 2 \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS_CFG, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS_CFG, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS_CFG, pin_memory=True if DEVICE.type == 'cuda' else False)\n",
    "\n",
    "print(\"\\nClass distribution in splits:\")\n",
    "for split_name, labels_in_split_arg in [('Train', y_train_labels), ('Validation', y_val_labels), ('Test', y_test_labels)]:\n",
    "    counts = Counter(labels_in_split_arg)\n",
    "    print(f\"{split_name}:\")\n",
    "    for label_idx, count in sorted(counts.items()):\n",
    "        class_name_found = [name for name, idx in label_mapping.items() if idx == label_idx]\n",
    "        if class_name_found:\n",
    "            class_name = class_name_found[0]\n",
    "            print(f\"  {class_name} (ID {label_idx}): {count}\")\n",
    "        else:\n",
    "            print(f\"  Unknown Label ID {label_idx}: {count}\")\n",
    "\n",
    "# --- Model Instantiation and Training Execution ---\n",
    "model_instance = SignLanguageHandsModel(\n",
    "    num_classes=NUM_CLASSES, \n",
    "    in_keypoint_features=IN_KEYPOINT_FEATURES, \n",
    "    num_hand_keypoints=NUM_KP_PER_HAND, \n",
    "    d_model=D_MODEL, \n",
    "    nhead=N_HEAD, \n",
    "    num_layers=NUM_LAYERS\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model instantiated with {sum(p.numel() for p in model_instance.parameters() if p.requires_grad)} trainable parameters.\")\n",
    "\n",
    "trained_model_instance = None\n",
    "history_data = None\n",
    "if train_loader and val_loader and len(y_train_labels) > 0:\n",
    "    trained_model_instance, history_data = train_model(\n",
    "        model=model_instance,\n",
    "        current_train_loader=train_loader,\n",
    "        current_val_loader=val_loader,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        device=DEVICE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        current_y_train_labels=y_train_labels \n",
    "    )\n",
    "else:\n",
    "    print(\"Dataloaders or y_train_labels not defined/empty. Skipping training.\")\n",
    "\n",
    "# --- Evaluation on Test Set ---\n",
    "model_for_eval = None\n",
    "if trained_model_instance is not None:\n",
    "    print(\"\\nEvaluating the model trained in this session.\")\n",
    "    model_for_eval = trained_model_instance\n",
    "elif os.path.exists('best_sign_model_hands_only.pth'):\n",
    "    print(\"\\nLoading best saved model ('best_sign_model_hands_only.pth') for evaluation.\")\n",
    "    model_for_eval = SignLanguageHandsModel(\n",
    "        num_classes=NUM_CLASSES, in_keypoint_features=IN_KEYPOINT_FEATURES, \n",
    "        num_hand_keypoints=NUM_KP_PER_HAND, d_model=D_MODEL, \n",
    "        nhead=N_HEAD, num_layers=NUM_LAYERS\n",
    "    ).to(DEVICE)\n",
    "    try:\n",
    "        model_for_eval.load_state_dict(torch.load('best_sign_model_hands_only.pth', map_location=DEVICE))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading saved model: {e}\")\n",
    "        model_for_eval = None\n",
    "else:\n",
    "    print(\"No trained model from this session and no 'best_sign_model_hands_only.pth' found. Skipping evaluation.\")\n",
    "\n",
    "if model_for_eval and test_loader is not None:\n",
    "    if label_mapping:\n",
    "        evaluation_results = evaluate_model(model_for_eval, test_loader, DEVICE, label_mapping, NUM_CLASSES)\n",
    "    else:\n",
    "        print(\"Error: label_mapping not defined or empty. Cannot generate class names for evaluation report.\")\n",
    "else:\n",
    "    print(\"Model or test_loader not available for evaluation.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slr_env_py39",
   "language": "python",
   "name": "slr_env_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
