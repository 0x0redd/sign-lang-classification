{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU capability: {torch.cuda.get_device_capability(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b639b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install av opencv-python numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16aa7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import VideoFileClip, vfx\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae2d8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Configuration & Setup ---\n",
    "RAW_VIDEO_DATASET_PATH = \"SignLanguageDataset\"  # <--- UPDATE THIS PATH\n",
    "BASE_OUTPUT_PATH = \"SignLanguage_Processed_Data\" # <--- UPDATE THIS PATH (if desired)\n",
    "\n",
    "SPLIT_DATA_PATH = os.path.join(BASE_OUTPUT_PATH, \"video_splits\")\n",
    "AUGMENTED_TRAIN_VIDEOS_PATH = os.path.join(BASE_OUTPUT_PATH, \"augmented_train_videos\")\n",
    "KEYPOINTS_OUTPUT_PATH = os.path.join(BASE_OUTPUT_PATH, \"keypoints_for_model\")\n",
    "\n",
    "PREFERRED_GPU_ENCODER = 'h264_nvenc' # TRY NVIDIA ENCODER or set to \"\" for CPU only\n",
    "CPU_FALLBACK_ENCODER = 'libx264'\n",
    "AUDIO_CODEC = 'aac'\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(SPLIT_DATA_PATH, exist_ok=True)\n",
    "os.makedirs(AUGMENTED_TRAIN_VIDEOS_PATH, exist_ok=True)\n",
    "os.makedirs(KEYPOINTS_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "N_FRAMES_KEYPOINTS = 30\n",
    "MOVEMENT_THRESHOLD_KEYPOINTS = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed109a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Dataset Splitting ---\n",
    "def split_video_dataset(\n",
    "    raw_dataset_path, output_split_path, train_ratio=0.7, val_ratio=0.15,\n",
    "    test_ratio=0.15, random_seed=42, move_files=False\n",
    "):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "    random.seed(random_seed)\n",
    "    if not os.path.exists(raw_dataset_path):\n",
    "        print(f\"Error: Raw dataset path '{raw_dataset_path}' does not exist.\")\n",
    "        return False\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(output_split_path, split), exist_ok=True)\n",
    "\n",
    "    print(f\"Starting dataset split from '{raw_dataset_path}' into '{output_split_path}'...\")\n",
    "    for class_name in os.listdir(raw_dataset_path):\n",
    "        class_dir = os.path.join(raw_dataset_path, class_name)\n",
    "        if not os.path.isdir(class_dir): continue\n",
    "\n",
    "        video_files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f)) and f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "        if not video_files:\n",
    "            print(f\"Warning: No video files found in class '{class_name}'.\")\n",
    "            continue\n",
    "        \n",
    "        random.shuffle(video_files)\n",
    "        n_total = len(video_files)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        n_test = n_total - n_train - n_val\n",
    "        if n_test < 0: n_test = 0\n",
    "\n",
    "        splits_data = {\n",
    "            'train': video_files[:n_train],\n",
    "            'val': video_files[n_train : n_train + n_val],\n",
    "            'test': video_files[n_train + n_val :]\n",
    "        }\n",
    "        \n",
    "        current_assigned_count = sum(len(v) for v in splits_data.values())\n",
    "        if current_assigned_count < n_total:\n",
    "            remaining_files = video_files[current_assigned_count:]\n",
    "            if len(splits_data['test']) < n_test and test_ratio > 0:\n",
    "                 splits_data['test'].extend(remaining_files)\n",
    "            elif len(splits_data['val']) < n_val and val_ratio > 0:\n",
    "                 splits_data['val'].extend(remaining_files)\n",
    "            else:\n",
    "                 splits_data['train'].extend(remaining_files)\n",
    "\n",
    "        for split_name, files_in_split in splits_data.items():\n",
    "            dest_class_dir = os.path.join(output_split_path, split_name, class_name)\n",
    "            os.makedirs(dest_class_dir, exist_ok=True)\n",
    "            for file_name in files_in_split:\n",
    "                src_file = os.path.join(class_dir, file_name)\n",
    "                dest_file = os.path.join(dest_class_dir, file_name)\n",
    "                if move_files: shutil.move(src_file, dest_file)\n",
    "                else: shutil.copy2(src_file, dest_file)\n",
    "        \n",
    "        print(f\"Class '{class_name}' split: Train={len(splits_data['train'])}, Val={len(splits_data['val'])}, Test={len(splits_data['test'])}\")\n",
    "    print(\"✅ Dataset splitting complete.\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3cc7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Video Augmentation ---\n",
    "def rotate_video(clip, angle): return clip.fx(vfx.rotate, angle)\n",
    "def zoom_video(clip, zoom_factor):\n",
    "    w, h = clip.size\n",
    "    new_w, new_h = int(w / zoom_factor), int(h / zoom_factor)\n",
    "    x1, y1 = (w - new_w) // 2, (h - new_h) // 2\n",
    "    return clip.fx(vfx.crop, x1=x1, y1=y1, width=new_w, height=new_h).resize(clip.size)\n",
    "def flip_horizontal_video(clip): return clip.fx(vfx.mirror_x)\n",
    "def add_gaussian_noise_vfx(clip, sigma=25):\n",
    "    def apply_noise(gf, t):\n",
    "        f = gf(t); n = np.random.normal(0, sigma, f.shape).astype(np.uint8); return cv2.add(f, n)\n",
    "    return clip.fl(apply_noise)\n",
    "def change_brightness_contrast_vfx(clip, bf=1.2, cf=1.2):\n",
    "    def apply_bc(gf,t): f=gf(t); a=cf; b=(bf-1)*127; return cv2.convertScaleAbs(f,alpha=a,beta=b)\n",
    "    return clip.fl(apply_bc)\n",
    "def translate_video_vfx(clip, dx, dy):\n",
    "    def apply_trans(gf,t): f=gf(t); M=np.float32([[1,0,dx],[0,1,dy]]); return cv2.warpAffine(f,M,(f.shape[1],f.shape[0]))\n",
    "    return clip.fl(apply_trans)\n",
    "def add_blur_vfx(clip, ks=(5,5)):\n",
    "    def apply_blur(gf,t): return cv2.GaussianBlur(gf(t),ks,0)\n",
    "    return clip.fl(apply_blur)\n",
    "def color_jitter_video(clip, br=0.2, co=0.2, sa=0.2, hu=0.1):\n",
    "    def apply_cj(gf,t):\n",
    "        f=gf(t); hsv=cv2.cvtColor(f,cv2.COLOR_RGB2HSV); h,s,v_=cv2.split(hsv)\n",
    "        s=np.clip(s*(1+random.uniform(-sa,sa)),0,255).astype(np.uint8)\n",
    "        h=np.clip(h+random.uniform(-hu*180,hu*180),0,179).astype(np.uint8)\n",
    "        fm=cv2.cvtColor(cv2.merge([h,s,v_]),cv2.COLOR_HSV2RGB)\n",
    "        a=1.0+random.uniform(-co,co); b_v=random.uniform(-br,br)*255\n",
    "        return cv2.convertScaleAbs(fm,alpha=a,beta=b_v)\n",
    "    return clip.fl(apply_cj)\n",
    "\n",
    "AUGMENTATIONS = {\n",
    "    \"rotation\": lambda clip: rotate_video(clip, random.uniform(-10, 10)),\n",
    "    \"zoom\": lambda clip: zoom_video(clip, random.uniform(1.05, 1.2)),\n",
    "    # \"flip_horizontal\": lambda clip: flip_horizontal_video(clip), # Often not useful for SL\n",
    "    \"gaussian_noise\": lambda clip: add_gaussian_noise_vfx(clip, sigma=random.uniform(10, 20)), # Reduced sigma\n",
    "    \"brightness_contrast\": lambda clip: change_brightness_contrast_vfx(clip, bf=random.uniform(0.9, 1.1), cf=random.uniform(0.9, 1.1)), # Reduced factors\n",
    "    \"translation\": lambda clip: translate_video_vfx(clip, dx=random.randint(-10, 10), dy=random.randint(-10, 10)), # Reduced translation\n",
    "    \"blur\": lambda clip: add_blur_vfx(clip, ks=(random.choice([3, 5]), random.choice([3, 5]))),\n",
    "    \"color_jitter\": lambda clip: color_jitter_video(clip, br=0.1, co=0.1, sa=0.1, hu=0.05), # Reduced jitter\n",
    "}\n",
    "\n",
    "def augment_video_set(train_video_input_path, augmented_output_path):\n",
    "    os.makedirs(augmented_output_path, exist_ok=True)\n",
    "    print(f\"Starting video augmentation for videos in: {train_video_input_path}\")\n",
    "    total_processed, total_augmented = 0, 0\n",
    "    for class_name in os.listdir(train_video_input_path):\n",
    "        class_path = os.path.join(train_video_input_path, class_name)\n",
    "        if not os.path.isdir(class_path): continue\n",
    "        output_class_path = os.path.join(augmented_output_path, class_name)\n",
    "        os.makedirs(output_class_path, exist_ok=True)\n",
    "        print(f\"\\nAugmenting class: {class_name}\")\n",
    "        video_files = [f for f in os.listdir(class_path) if f.lower().endswith(('.mp4', '.avi', '.mov'))]\n",
    "        for video_file in tqdm(video_files, desc=f\"Videos in {class_name}\"):\n",
    "            video_path = os.path.join(class_path, video_file); base_name, ext = os.path.splitext(video_file)\n",
    "            output_original_path = os.path.join(output_class_path, video_file)\n",
    "            try: shutil.copy2(video_path, output_original_path); total_processed += 1\n",
    "            except Exception as e: print(f\"Error copying {video_file}: {e}\"); continue\n",
    "            for aug_name, aug_func in AUGMENTATIONS.items():\n",
    "                output_filename = f\"{base_name}_{aug_name}{ext}\"\n",
    "                output_aug_path = os.path.join(output_class_path, output_filename)\n",
    "                if os.path.exists(output_aug_path): continue # Skip if already augmented\n",
    "                try:\n",
    "                    clip = VideoFileClip(video_path); augmented_clip = aug_func(clip.copy())\n",
    "                    encoder_params = {'codec': CPU_FALLBACK_ENCODER, 'audio_codec': AUDIO_CODEC, 'logger': None, 'threads': 4, 'preset': 'ultrafast'}\n",
    "                    if PREFERRED_GPU_ENCODER:\n",
    "                        try:\n",
    "                            gpu_params = encoder_params.copy(); gpu_params['codec'] = PREFERRED_GPU_ENCODER; del gpu_params['preset']\n",
    "                            augmented_clip.write_videofile(output_aug_path, **gpu_params)\n",
    "                        except Exception: augmented_clip.write_videofile(output_aug_path, **encoder_params)\n",
    "                    else: augmented_clip.write_videofile(output_aug_path, **encoder_params)\n",
    "                    clip.close(); augmented_clip.close(); total_augmented += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError: {aug_name} for {video_file}: {e}\")\n",
    "                    if os.path.exists(output_aug_path): os.remove(output_aug_path)\n",
    "    print(f\"\\n--- Video Augmentation Complete ---\\nOriginals: {total_processed}, Augmentations: {total_augmented}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2709e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Keypoint Extraction ---\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "def extract_holistic_landmarks(results):\n",
    "    def get_coords(lc, es):\n",
    "        if lc and lc.landmark: return np.array([[lm.x,lm.y,lm.z] for lm in lc.landmark], dtype=np.float32)\n",
    "        return np.zeros((es,3),dtype=np.float32)\n",
    "    return np.concatenate([get_coords(results.pose_landmarks,33), get_coords(results.face_landmarks,468),\n",
    "                           get_coords(results.left_hand_landmarks,21), get_coords(results.right_hand_landmarks,21)], axis=0)\n",
    "def get_hand_center(hl): return np.array([hl.landmark[0].x,hl.landmark[0].y,hl.landmark[0].z],dtype=np.float32) if hl and hl.landmark else None\n",
    "def hands_moved(pl,pr,cl,cr,th):\n",
    "    if any(x is None for x in [pl,pr,cl,cr]): return False\n",
    "    return (np.linalg.norm(cl-pl)>th or np.linalg.norm(cr-pr)>th)\n",
    "\n",
    "def process_video_for_keypoints(video_path, n_frames=N_FRAMES_KEYPOINTS, movement_threshold=MOVEMENT_THRESHOLD_KEYPOINTS):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened(): print(f\"Error: Vid {video_path}\"); return np.zeros((n_frames,543,3),dtype=np.float32)\n",
    "    seq_kps, m_start, plh, prh, f_cnt_mov, p_f_cnt = [], False, None, None, 0, 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        p_f_cnt+=1; img_rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB); results=holistic_model.process(img_rgb)\n",
    "        clh,crh=get_hand_center(results.left_hand_landmarks),get_hand_center(results.right_hand_landmarks)\n",
    "        if not m_start:\n",
    "            if hands_moved(plh,prh,clh,crh,movement_threshold): m_start=True\n",
    "            plh,prh=clh,crh; continue\n",
    "        f_cnt_mov+=1\n",
    "        if f_cnt_mov%2==0: seq_kps.append(extract_holistic_landmarks(results))\n",
    "        if len(seq_kps)==n_frames: break\n",
    "    cap.release()\n",
    "    if not seq_kps:\n",
    "        print(f\"Warn: No kps for '{video_path}'. Frames: {p_f_cnt}.\")\n",
    "        return np.zeros((n_frames,543,3),dtype=np.float32)\n",
    "    if len(seq_kps)<n_frames: seq_kps.extend([seq_kps[-1]]*(n_frames-len(seq_kps)))\n",
    "    return np.array(seq_kps[:n_frames])\n",
    "\n",
    "def extract_keypoints_from_video_set(video_set_input_path, keypoints_root_output_dir, split_name):\n",
    "    print(f\"\\nKeypoint extraction for '{split_name}' set from: {video_set_input_path}\")\n",
    "    if not os.path.exists(video_set_input_path): print(f\"Error: Path '{video_set_input_path}' DNE.\"); return\n",
    "    for class_name in os.listdir(video_set_input_path):\n",
    "        videos_in_class_path = os.path.join(video_set_input_path, class_name)\n",
    "        if not os.path.isdir(videos_in_class_path): continue\n",
    "        keypoints_out_class_split = os.path.join(keypoints_root_output_dir,class_name,split_name)\n",
    "        os.makedirs(keypoints_out_class_split,exist_ok=True)\n",
    "        print(f\"Processing class: {class_name} ({split_name})\")\n",
    "        vfs = [f for f in os.listdir(videos_in_class_path) if f.lower().endswith(('.mp4','.avi','.mov'))]\n",
    "        for vf in tqdm(vfs, desc=f\"Keypoints {class_name} ({split_name})\"):\n",
    "            vfp=os.path.join(videos_in_class_path,vf); bn,_=os.path.splitext(vf)\n",
    "            onfp=os.path.join(keypoints_out_class_split,f\"{bn}.npy\")\n",
    "            if os.path.exists(onfp): continue\n",
    "            np.save(onfp, process_video_for_keypoints(vfp))\n",
    "    print(f\"✅ Keypoint extraction for '{split_name}' set complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Split\n",
    "    print(\"--- STEP 1: Splitting Dataset ---\")\n",
    "    train_dir = os.path.join(SPLIT_DATA_PATH, \"train\")\n",
    "    if not (os.path.exists(train_dir) and os.listdir(train_dir)):\n",
    "        if not split_video_dataset(RAW_VIDEO_DATASET_PATH, SPLIT_DATA_PATH):\n",
    "            print(\"Halting due to error in video dataset splitting.\"); exit()\n",
    "    else: print(f\"Video splits found at '{SPLIT_DATA_PATH}'. Skipping.\")\n",
    "\n",
    "    # Step 2: Augment\n",
    "    print(\"\\n--- STEP 2: Augmenting Training Videos ---\")\n",
    "    train_split_path_for_aug = os.path.join(SPLIT_DATA_PATH, \"train\")\n",
    "    aug_train_dir_has_content = os.path.exists(AUGMENTED_TRAIN_VIDEOS_PATH) and any(os.scandir(AUGMENTED_TRAIN_VIDEOS_PATH))\n",
    "    if not aug_train_dir_has_content :\n",
    "         if os.path.exists(train_split_path_for_aug) and any(os.scandir(train_split_path_for_aug)):\n",
    "            augment_video_set(train_split_path_for_aug, AUGMENTED_TRAIN_VIDEOS_PATH)\n",
    "         else: print(f\"Skipping augmentation: Training split path '{train_split_path_for_aug}' empty/DNE.\")\n",
    "    else: print(f\"Augmented videos found at '{AUGMENTED_TRAIN_VIDEOS_PATH}'. Skipping.\")\n",
    "\n",
    "    # Step 3: Extract Keypoints\n",
    "    print(\"\\n--- STEP 3: Extracting Keypoints ---\")\n",
    "    kp_train_dir_example = os.path.join(KEYPOINTS_OUTPUT_PATH, (os.listdir(RAW_VIDEO_DATASET_PATH)[0] if os.path.exists(RAW_VIDEO_DATASET_PATH) and os.listdir(RAW_VIDEO_DATASET_PATH) else \"dummy\"), \"train\")\n",
    "    \n",
    "    keypoints_exist = os.path.exists(kp_train_dir_example) and any(os.scandir(kp_train_dir_example))\n",
    "    \n",
    "    if not keypoints_exist:\n",
    "        source_for_train_kp = AUGMENTED_TRAIN_VIDEOS_PATH if (os.path.exists(AUGMENTED_TRAIN_VIDEOS_PATH) and any(os.scandir(AUGMENTED_TRAIN_VIDEOS_PATH))) else train_split_path_for_aug\n",
    "        if os.path.exists(source_for_train_kp) and any(os.scandir(source_for_train_kp)):\n",
    "            print(f\"Extracting train keypoints from: {source_for_train_kp}\")\n",
    "            extract_keypoints_from_video_set(source_for_train_kp, KEYPOINTS_OUTPUT_PATH, \"train\")\n",
    "        else: print(f\"Source for train keypoints ('{source_for_train_kp}') empty/DNE.\")\n",
    "\n",
    "        val_split_path_for_kp = os.path.join(SPLIT_DATA_PATH, \"val\")\n",
    "        if os.path.exists(val_split_path_for_kp) and any(os.scandir(val_split_path_for_kp)):\n",
    "            extract_keypoints_from_video_set(val_split_path_for_kp, KEYPOINTS_OUTPUT_PATH, \"val\")\n",
    "        else: print(f\"Val split path '{val_split_path_for_kp}' empty/DNE for keypoints.\")\n",
    "\n",
    "        test_split_path_for_kp = os.path.join(SPLIT_DATA_PATH, \"test\")\n",
    "        if os.path.exists(test_split_path_for_kp) and any(os.scandir(test_split_path_for_kp)):\n",
    "            extract_keypoints_from_video_set(test_split_path_for_kp, KEYPOINTS_OUTPUT_PATH, \"test\")\n",
    "        else: print(f\"Test split path '{test_split_path_for_kp}' empty/DNE for keypoints.\")\n",
    "    else:\n",
    "        print(f\"Keypoints seem extracted in '{KEYPOINTS_OUTPUT_PATH}'. Skipping.\")\n",
    "        \n",
    "    if 'holistic_model' in globals(): holistic_model.close() \n",
    "    print(\"\\n--- ALL PREPROCESSING STEPS ATTEMPTED ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slr_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
