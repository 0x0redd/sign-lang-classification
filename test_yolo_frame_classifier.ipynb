{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eecbdd5a",
   "metadata": {},
   "source": [
    "# yolo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_DATASET_ROOT = os.path.join(\"SignLanguage_Processed_Frames\", \"YOLO_Frames_Dataset\")\n",
    "TEST_SPLIT_DIR = os.path.join(YOLO_DATASET_ROOT, \"test\")\n",
    "\n",
    "# Update this path to where your 'best.pt' is located from your training run\n",
    "MODEL_PATH = \"runs/slr_yolo_frame_cls_train/exp_frames/weights/best.pt\" \n",
    "\n",
    "IMG_SIZE = 224 \n",
    "BATCH_SIZE_TEST = 32 \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_score_model(model_path, test_data_dir, img_size, batch_size):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"ERROR: Trained model not found at {model_path}\")\n",
    "        return None\n",
    "    if not os.path.exists(test_data_dir):\n",
    "        print(f\"ERROR: Test data directory not found at {test_data_dir}\")\n",
    "        return None\n",
    "\n",
    "    model = YOLO(model_path)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    print(f\"\\n--- Evaluating Model on Test Set ---\")\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"Test Data: {test_data_dir}\")\n",
    "    print(f\"Image Size: {img_size}\")\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "\n",
    "    all_preds_indices = []\n",
    "    all_true_labels_indices = []\n",
    "    class_names_from_model = model.names\n",
    "    class_to_idx = {name: idx for idx, name in class_names_from_model.items()}\n",
    "\n",
    "    print(\"Collecting predictions on the test set...\")\n",
    "    for class_name in tqdm(os.listdir(test_data_dir), desc=\"Processing classes\"):\n",
    "        class_dir_path = os.path.join(test_data_dir, class_name)\n",
    "        if not os.path.isdir(class_dir_path): continue\n",
    "        if class_name not in class_to_idx: \n",
    "            print(f\"Warning: Class '{class_name}' from test set not in model's classes. Skipping.\"); continue\n",
    "        true_label_idx = class_to_idx[class_name]\n",
    "        image_files = [os.path.join(class_dir_path, f) for f in os.listdir(class_dir_path) \n",
    "                       if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        if not image_files: continue\n",
    "\n",
    "        for i in range(0, len(image_files), batch_size):\n",
    "            batch_files = image_files[i:i+batch_size]\n",
    "            try:\n",
    "                results = model.predict(source=batch_files, imgsz=img_size, verbose=False, device=DEVICE, stream=False)\n",
    "                for res in results:\n",
    "                    if hasattr(res, 'probs') and res.probs is not None:\n",
    "                        pred_idx = res.probs.top1\n",
    "                        all_preds_indices.append(pred_idx)\n",
    "                        all_true_labels_indices.append(true_label_idx)\n",
    "                    else:\n",
    "                        print(f\"Warning: No probabilities for an image in {class_name}. Result obj: {type(res)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction for batch in {class_name}: {e}\")\n",
    "\n",
    "    if not all_true_labels_indices: print(\"No predictions were made.\"); return None\n",
    "\n",
    "    num_model_classes = len(class_names_from_model)\n",
    "    target_names_for_report_cm = [class_names_from_model[i] for i in range(num_model_classes)]\n",
    "    unique_true_labels_indices = sorted(list(set(all_true_labels_indices)))\n",
    "    target_names_report_only = [class_names_from_model[i] for i in unique_true_labels_indices]\n",
    "\n",
    "    print(\"\\nüìä Classification Report (per frame):\")\n",
    "    print(classification_report(\n",
    "        all_true_labels_indices, \n",
    "        all_preds_indices, \n",
    "        labels=unique_true_labels_indices, \n",
    "        target_names=target_names_report_only, \n",
    "        digits=4, \n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    accuracy = accuracy_score(all_true_labels_indices, all_preds_indices)\n",
    "    f1_macro = f1_score(all_true_labels_indices, all_preds_indices, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(all_true_labels_indices, all_preds_indices, average='weighted', zero_division=0)\n",
    "\n",
    "    print(f\"\\nüîç Overall Test Accuracy (per frame): {accuracy:.4f}\")\n",
    "    print(f\"üîç Overall Test F1-Score (Macro, per frame): {f1_macro:.4f}\")\n",
    "    print(f\"üîç Overall Test F1-Score (Weighted, per frame): {f1_weighted:.4f}\")\n",
    "\n",
    "    cm = confusion_matrix(all_true_labels_indices, all_preds_indices, labels=list(range(num_model_classes)))\n",
    "    plt.figure(figsize=(max(8, num_model_classes), max(6, num_model_classes * 0.8)))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_names_for_report_cm,\n",
    "                yticklabels=target_names_for_report_cm)\n",
    "    plt.title(\"Confusion Matrix (Per Frame on Test Set)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix_yolo_frames_test.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"f1_weighted\": f1_weighted,\n",
    "        \"classification_report\": classification_report(\n",
    "            all_true_labels_indices, all_preds_indices, \n",
    "            labels=unique_true_labels_indices, target_names=target_names_report_only, \n",
    "            digits=4, zero_division=0, output_dict=True\n",
    "        ),\n",
    "        \"confusion_matrix\": cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Execute Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_and_score_model(\n",
    "    model_path=MODEL_PATH,\n",
    "    test_data_dir=TEST_SPLIT_DIR,\n",
    "    img_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE_TEST\n",
    ")\n",
    "if results:\n",
    "    print(\"\\nEvaluation Finished.\")\n",
    "else:\n",
    "    print(\"\\nEvaluation could not be completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slr_env_py39",
   "language": "python",
   "name": "slr_env_py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
